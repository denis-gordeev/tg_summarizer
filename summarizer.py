import os
import asyncio
import json
from datetime import datetime, timedelta, timezone
from difflib import SequenceMatcher
import re
from dataclasses import dataclass
from typing import List, Set

from dotenv import load_dotenv
from telethon import TelegramClient
from openai import OpenAI

load_dotenv()

# Get environment variables with proper error handling
api_id_str = os.getenv('TELEGRAM_API_ID')
if not api_id_str:
    raise ValueError("TELEGRAM_API_ID environment variable is required")
API_ID = int(api_id_str)

api_hash = os.getenv('TELEGRAM_API_HASH')
if not api_hash:
    raise ValueError("TELEGRAM_API_HASH environment variable is required")
API_HASH = api_hash

bot_token = os.getenv('TELEGRAM_BOT_TOKEN')
if not bot_token:
    raise ValueError("TELEGRAM_BOT_TOKEN environment variable is required")
BOT_TOKEN = bot_token

target_channel = os.getenv('TARGET_CHANNEL')
if not target_channel:
    raise ValueError("TARGET_CHANNEL environment variable is required")
TARGET_CHANNEL = target_channel

openai_api_key = os.getenv('OPENAI_API_KEY')
if not openai_api_key:
    raise ValueError("OPENAI_API_KEY environment variable is required")
OPENAI_API_KEY = openai_api_key

source_channels_str = os.getenv('SOURCE_CHANNELS', '')
SOURCE_CHANNELS = [c.strip() for c in source_channels_str.split(',') if c.strip()]

openai_client = OpenAI(api_key=OPENAI_API_KEY)

LINK_REGEX = re.compile(r"https?://\S+")

# Create separate clients for user (reading) and bot (sending)
user_client = TelegramClient('tg_summarizer_user', API_ID, API_HASH)
bot_client = TelegramClient('tg_summarizer_bot', API_ID, API_HASH)

SIMILARITY_THRESHOLD = 0.9
HISTORY_FILE = 'summarization_history.json'


@dataclass
class MessageInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–æ–æ–±—â–µ–Ω–∏–∏ Telegram"""
    text: str
    channel: str
    message_id: int
    date: datetime
    link: str
    
    def get_telegram_link(self) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—Å—ã–ª–∫—É –Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ Telegram"""
        # –£–±–∏—Ä–∞–µ–º @ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–∞–Ω–∞–ª–∞ –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è —Å—Å—ã–ª–∫–∏
        channel_name = self.channel.lstrip('@')
        return f"https://t.me/{channel_name}/{self.message_id}"
    
    def to_dict(self) -> dict:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –æ–±—ä–µ–∫—Ç –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ JSON"""
        return {
            'text': self.text,
            'channel': self.channel,
            'message_id': self.message_id,
            'date': self.date.isoformat(),
            'link': self.link
        }
    
    @classmethod
    def from_dict(cls, data: dict) -> 'MessageInfo':
        """–°–æ–∑–¥–∞–µ—Ç –æ–±—ä–µ–∫—Ç –∏–∑ —Å–ª–æ–≤–∞—Ä—è"""
        return cls(
            text=data['text'],
            channel=data['channel'],
            message_id=data['message_id'],
            date=datetime.fromisoformat(data['date']),
            link=data['link']
        )


async def call_openai(system_prompt: str, user_content: str, max_tokens: int = 300) -> str:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–∑–æ–≤–∞ OpenAI API."""
    try:
        response = openai_client.chat.completions.create(
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ],
            model="gpt-4o-mini",
            max_tokens=max_tokens,
        )
        result = response.choices[0].message.content
        if result is None:
            return ""
        return result.strip()
    except Exception as e:
        print(f"OpenAI API error: {e}")
        return ""


def extract_links(text: str) -> list[str]:
    """Return all URLs from a string."""
    return LINK_REGEX.findall(text)


def load_summarization_history() -> Set[str]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ —Ñ–∞–π–ª–∞."""
    if not os.path.exists(HISTORY_FILE):
        return set()
    
    try:
        with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
            processed_messages = set()
            for msg_data in data.get('processed_messages', []):
                msg = MessageInfo.from_dict(msg_data)
                # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä: –∫–∞–Ω–∞–ª + message_id + —Ö–µ—à —Ç–µ–∫—Å—Ç–∞
                msg_id = f"{msg.channel}_{msg.message_id}_{hash(msg.text)}"
                processed_messages.add(msg_id)
            return processed_messages
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏—Å—Ç–æ—Ä–∏–∏: {e}")
        return set()


def save_summarization_history(messages: List[MessageInfo]) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –∏—Å—Ç–æ—Ä–∏—é."""
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∏—Å—Ç–æ—Ä–∏—é
        existing_data = []
        if os.path.exists(HISTORY_FILE):
            with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                existing_data = data.get('processed_messages', [])
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        new_messages = [msg.to_dict() for msg in messages]
        all_messages = existing_data + new_messages
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏ 1000 —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏
        if len(all_messages) > 1000:
            all_messages = all_messages[-1000:]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é
        with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
            json.dump({
                'processed_messages': all_messages,
                'last_updated': datetime.now().isoformat()
            }, f, ensure_ascii=False, indent=2)
            
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∏—Å—Ç–æ—Ä–∏–∏: {e}")


def is_message_processed(msg: MessageInfo, processed_messages: Set[str]) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –±—ã–ª–æ –ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ä–∞–Ω–µ–µ."""
    msg_id = f"{msg.channel}_{msg.message_id}_{hash(msg.text)}"
    return msg_id in processed_messages


async def are_messages_duplicate(msg_a: MessageInfo, msg_b: MessageInfo) -> bool:
    """Use the LLM to see if two messages cover the same topic."""
    system_prompt = (
        "–û–ø–∏—Å—ã–≤–∞—é—Ç –ª–∏ —Å–ª–µ–¥—É—é—â–∏–µ –¥–≤–∞ —Å–æ–æ–±—â–µ–Ω–∏—è Telegram –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –∏–ª–∏ —Å—Ç–∞—Ç—å—é?\n"
        "–û—Ç–≤–µ—Ç—å—Ç–µ –¥–∞ –∏–ª–∏ –Ω–µ—Ç."
    )
    user_content = f"Message 1:\n{msg_a.text}\n\nMessage 2:\n{msg_b.text}"
    
    answer = await call_openai(system_prompt, user_content, max_tokens=1)
    return answer.lower().startswith("y")


async def is_nlp_related(text: str) -> bool:
    """Use the LLM to decide if a message is NLP related and not advertising."""
    system_prompt = (
        "–í—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä. –û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º –¥–ª—è NLP/ML –¥–∞–π–¥–∂–µ—Å—Ç–∞.\n\n"
        "–ü–†–ò–ù–ò–ú–ê–ô–¢–ï (–æ—Ç–≤–µ—Ç—å—Ç–µ '–¥–∞'):\n"
        "- –ù–∞—É—á–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è\n"
        "- –ù–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –±–∏–±–ª–∏–æ—Ç–µ–∫–∏, –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã\n"
        "- –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –æ–±–∑–æ—Ä—ã –∏ –±–µ–Ω—á–º–∞—Ä–∫–∏\n"
        "- –ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏ –∏ –≤–æ—Ä–∫—à–æ–ø—ã\n"
        "- –û—Ç–∫—Ä—ã—Ç—ã–µ –ø—Ä–æ–µ–∫—Ç—ã –∏ –¥–∞—Ç–∞—Å–µ—Ç—ã\n"
        "- –ù–æ–≤–æ—Å—Ç–∏ –ø—Ä–æ Kaggle, —Ö–∞–∫–∞—Ç–æ–Ω—ã –∏ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è\n"
        "- –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏ —Å–æ–≤–µ—Ç—ã –æ –∫–∞—Ä—å–µ—Ä–µ –∏ –≤–∞–∫–∞–Ω—Å–∏—è—Ö\n"
        "- –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–µ—Ç-–ø—Ä–æ–µ–∫—Ç–∞—Ö\n"
        "- –¢–µ–∫—Å—Ç—ã –ø–æ –∏–∏ –∏ bigtech –∫–æ–º–ø–∞–Ω–∏–∏\n"
        "- –õ—é–±—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –ø—Ä–æ –°—ç–º–∞ –ê–ª—å—Ç–º–∞–Ω–∞ (Sam Altman), "
        "OpenAI, Anthropic, Google, Meta, Microsoft, Nvidia, FAANG –∏ —Ç.–¥.\n"
        "- –ù–æ–≤–æ—Å—Ç–∏ –ø—Ä–æ –ø–æ–∫—É–ø–∫—É, –ø—Ä–æ–¥–∞–∂—É –∫–æ–º–ø–∞–Ω–∏–π –∏ –ø–æ–≥–ª–æ—â–µ–Ω–∏—è\n:"
        "- –ù–æ–≤–æ—Å—Ç–∏ –ø—Ä–æ —É–≤–æ–ª—å–Ω–µ–Ω–∏—è –∏ –ò–ò-—ç–∫–æ–Ω–æ–º–∏–∫—É\n:"
        "- –ù–æ–≤–æ—Å—Ç–∏ –ø—Ä–æ GPU –∏ –∂–µ–ª–µ–∑–æ\n:"
        "- –ù–æ–≤–æ—Å—Ç–∏ –ø—Ä–æ –≤–∞–π–±–∫–æ–¥–∏–Ω–≥ (vibe coding) –∏ –∏–∏-–∞–≥–µ–Ω—Ç–æ–≤ (—á–∞—Å—Ç–æ –ø—Ä–æ—Å—Ç–æ –∞–≥–µ–Ω—Ç—ã, agents)\n:"

        "–û–¢–ö–õ–û–ù–Ø–ô–¢–ï (–æ—Ç–≤–µ—Ç—å—Ç–µ '–Ω–µ—Ç'):\n"
        "- –ö—É—Ä—Å—ã, –æ–±—É—á–µ–Ω–∏–µ, –ø–ª–∞—Ç–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã\n"
        "- –†–µ–∫–ª–∞–º–∞ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö LLM (GigaChat, YandexGPT)\n"
        "- –ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ —É—Å–ª—É–≥–∏\n"
        "- –í–µ–±–∏–Ω–∞—Ä—ã —Å –ø—Ä–æ–¥–∞–∂–∞–º–∏\n"
        "- –ú–∞—Å—Ç–µ—Ä-–∫–ª–∞—Å—Å—ã —Å —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–∞–º–∏\n"
        "- Hiring days\n"
        "–û—Ç–≤–µ—á–∞–π—Ç–µ —Ç–æ–ª—å–∫–æ '–¥–∞' –∏–ª–∏ '–Ω–µ—Ç'."
    )
    
    answer = await call_openai(system_prompt, text, max_tokens=5)
    return answer.lower().strip().startswith('–¥–∞')


async def summarize_text(messages: List[MessageInfo]) -> str:
    """Call LLM to summarize the given messages with links."""
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –Ω–æ–º–µ—Ä–æ–≤ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    messages_with_sources = []
    for i, msg in enumerate(messages, 1):
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å–æ–æ–±—â–µ–Ω–∏—è
        links = extract_links(msg.text)
        source_info = f"[{i}] {msg.text}"
        if links:
            source_info += f" (–°—Å—ã–ª–∫–∏: {', '.join(links)})"
        messages_with_sources.append(source_info)
    
    messages_text = "\n\n".join(messages_with_sources)
    
    system_prompt = (
        "–û–±–æ–±—â–∏—Ç–µ —Å–ª–µ–¥—É—é—â–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è Telegram –≤ –∫—Ä–∞—Ç–∫–∏–π –µ–∂–µ–¥–Ω–µ–≤–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç, —Å—Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ NLP. "
        "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π—Ç–µ –¥–∞–π–¥–∂–µ—Å—Ç –ø–æ —Ç–µ–º–∞–º –∏–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –±–ª–æ–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —É–∫–∞–∑—ã–≤–∞–π—Ç–µ "
        "–Ω–æ–º–µ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –≤ –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã—Ö —Å–∫–æ–±–∫–∞—Ö [1], [2], [3] –∏ —Ç.–¥. –ï—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –≥–æ–≤–æ—Ä—è—Ç "
        "–æ–± –æ–¥–Ω–æ–º –∏ —Ç–æ–º –∂–µ, —Ç–æ —É–∫–∞–∑—ã–≤–∞–π—Ç–µ –≤—Å–µ. "
        "–î–ª—è —Å—Ç–∞—Ç–µ–π –∏ –Ω–æ–≤—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –∫—Ä–∞—Ç–∫–æ –æ–ø–∏—à–∏—Ç–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã: –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã. "
        "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∂–∏—Ä–Ω—ã–π —à—Ä–∏—Ñ—Ç <b>—Ç–µ–∫—Å—Ç</b> –∏ —Å–º–∞–π–ª–∏–∫–∏ –¥–ª—è –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–∫–æ–≤, –Ω–∞–ø—Ä–∏–º–µ—Ä: "
        "<b>üöÄ –ù–æ–≤—ã–µ –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏ –†–µ—Å—É—Ä—Å—ã –¥–ª—è –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π</b>. "
        "–í–ê–ñ–ù–û: –ù–ï —Å–æ–∑–¥–∞–≤–∞–π—Ç–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –≤ —Ç–µ–∫—Å—Ç–µ, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ–ª—å–∫–æ –Ω–æ–º–µ—Ä–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ [1], [2], [3] –∏ —Ç.–¥. "
        "–°—Å—ã–ª–∫–∏ –±—É–¥—É—Ç –¥–æ–±–∞–≤–ª–µ–Ω—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏."
    )
    
    result = await call_openai(system_prompt, messages_text, max_tokens=16000)
    if not result:
        return "–û—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ–±–æ–±—â–µ–Ω–∏–µ"
    
    # –ó–∞–º–µ–Ω—è–µ–º –Ω–æ–º–µ—Ä–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–∞ HTML-—Å—Å—ã–ª–∫–∏
    def replace_source_with_links(match):
        content = match.group(1)  # —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤–Ω—É—Ç—Ä–∏ —Å–∫–æ–±–æ–∫
        numbers = [num.strip() for num in content.split(',')]
        source_links = []
        
        for num_str in numbers:
            try:
                num = int(num_str)
                if 1 <= num <= len(messages):
                    msg = messages[num-1]
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å–æ–æ–±—â–µ–Ω–∏—è
                    links = extract_links(msg.text)
                    
                    if links:
                        # –ï—Å–ª–∏ –µ—Å—Ç—å —Å—Å—ã–ª–∫–∏ –≤ —Å–æ–æ–±—â–µ–Ω–∏–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—É—é –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω—É—é
                        main_link = links[0]
                        source_links.append(f'<a href="{main_link}">[{num}]</a>')
                    else:
                        # –ï—Å–ª–∏ —Å—Å—ã–ª–æ–∫ –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ Telegram-—Å–æ–æ–±—â–µ–Ω–∏–µ
                        telegram_link = msg.get_telegram_link()
                        source_links.append(f'<a href="{telegram_link}">[{num}]</a>')
                        
            except ValueError:
                continue
        return ', '.join(source_links)

    # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ [1], [1,2], [1,2,3] –∏ —Ç.–¥.
    all_sources_pattern = r'\[(\d+(?:,\s*\d+)*)\]'
    result = re.sub(all_sources_pattern, replace_source_with_links, result)

    return result


async def fetch_messages():
    """Fetch messages from source channels in the last 24 hours."""
    since = datetime.now(timezone.utc) - timedelta(days=1)
    all_msgs = []
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
    processed_messages = load_summarization_history()
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(processed_messages)} —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏")
    
    for channel in SOURCE_CHANNELS:
        print(f"Fetching messages from {channel}...")
        channel_msgs = []
        async for msg in user_client.iter_messages(channel, offset_date=None, min_id=0, reverse=False):
            if msg.date < since:
                break
            if msg.message:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å–æ–æ–±—â–µ–Ω–∏—è
                links = extract_links(msg.message)
                main_link = links[0] if links else ""
                
                message_info = MessageInfo(
                    text=msg.message,
                    channel=channel,
                    message_id=msg.id,
                    date=msg.date,
                    link=main_link
                )
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –±—ã–ª–æ –ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ
                if not is_message_processed(message_info, processed_messages):
                    channel_msgs.append(message_info)
                    all_msgs.append(message_info)
                else:
                    print(f"  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ {msg.id} –∏–∑ {channel}")
                    
        print(f"  Found {len(channel_msgs)} –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π from {channel}")
    return all_msgs


async def remove_duplicates(messages: List[MessageInfo]) -> List[MessageInfo]:
    unique_msgs: List[MessageInfo] = []
    seen_links = set()
    
    for msg in messages:
        links = extract_links(msg.text)
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ —Å—Å—ã–ª–∫–∞–º - –µ—Å–ª–∏ —Å—Å—ã–ª–∫–∞ —É–∂–µ –±—ã–ª–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        if links and any(link in seen_links for link in links):
            print(f"  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç –ø–æ —Å—Å—ã–ª–∫–µ: {links[0]}")
            continue
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ —Ç–µ–∫—Å—Ç—É
        duplicate = False
        for u in unique_msgs:
            if SequenceMatcher(None, msg.text, u.text).ratio() > SIMILARITY_THRESHOLD:
                print(f"  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç –ø–æ —Ç–µ–∫—Å—Ç—É: {msg.text[:50]}...")
                duplicate = True
                break
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –¥—É–±–ª–∏–∫–∞—Ç –ø–æ —Ç–µ–∫—Å—Ç—É, –ø—Ä–æ–≤–µ—Ä—è–µ–º —á–µ—Ä–µ–∑ LLM
        if not duplicate:
            for u in unique_msgs:
                try:
                    if await are_messages_duplicate(msg, u):
                        print(f"  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç –ø–æ LLM: {msg.text[:50]}...")
                        duplicate = True
                        break
                except Exception as e:
                    print(f"  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –¥—É–±–ª–∏–∫–∞—Ç–∞ —á–µ—Ä–µ–∑ LLM: {e}")
                    # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ LLM, —Å—á–∏—Ç–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è —Ä–∞–∑–Ω—ã–º–∏
                    continue
        
        if not duplicate:
            unique_msgs.append(msg)
            seen_links.update(links)
            print(f"  –î–æ–±–∞–≤–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ: {msg.text[:50]}...")
    
    return unique_msgs


async def main():
    # Start both clients
    await user_client.start()
    await bot_client.start(bot_token=BOT_TOKEN)
    
    try:
        messages = await fetch_messages()
        print(f"Fetched {len(messages)} new messages")
        
        if not messages:
            print("No new messages found")
            return
        
        # Apply NLP filtering to remove advertising
        filtered = []
        all_checked_messages = []  # –í—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã —á–µ—Ä–µ–∑ is_nlp_related
        
        for i, msg in enumerate(messages):
            print(f"Checking message {i+1}/{len(messages)}...")
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            all_checked_messages.append(msg)
            
            if await is_nlp_related(msg.text):
                filtered.append(msg)
                print(f"  ‚úì Message {i+1} is NLP-related: {msg.text[:100]}; {msg.link}")
            else:
                print(f"  ‚úó Message {i+1} is not NLP-related (likely advertising): {msg.text[:100]}; {msg.link}")
        print(f"{len(filtered)} messages after NLP filter")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –∏—Å—Ç–æ—Ä–∏—é
        save_summarization_history(all_checked_messages)
        print(f"Saved {len(all_checked_messages)} checked messages to history")
        
        if not filtered:
            print("No new NLP-related messages found")
            return
            
        unique = await remove_duplicates(filtered)
        print(f"{len(unique)} messages after deduplication")
        
        if not unique:
            print("No unique NLP messages found")
            return
            
        summary = await summarize_text(unique)
        await user_client.send_message(TARGET_CHANNEL, summary, parse_mode='html')
        print("Summary sent")
        
    finally:
        # Disconnect both clients
        await user_client.disconnect()
        await bot_client.disconnect()


if __name__ == "__main__":
    asyncio.run(main())
