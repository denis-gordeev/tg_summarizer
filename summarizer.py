import os
import asyncio
import json
from datetime import datetime, timedelta, timezone
from difflib import SequenceMatcher
import random
import re
from dataclasses import dataclass
from typing import List, Set

from dotenv import load_dotenv
from telethon import TelegramClient
from telethon.tl.functions.channels import GetChannelRecommendationsRequest
from telethon.tl.types import InputChannel
from openai import OpenAI

load_dotenv()

# Get environment variables with proper error handling
api_id_str = os.getenv('TELEGRAM_API_ID')
if not api_id_str:
    raise ValueError("TELEGRAM_API_ID environment variable is required")
API_ID = int(api_id_str)

api_hash = os.getenv('TELEGRAM_API_HASH')
if not api_hash:
    raise ValueError("TELEGRAM_API_HASH environment variable is required")
API_HASH = api_hash

bot_token = os.getenv('TELEGRAM_BOT_TOKEN')
if not bot_token:
    raise ValueError("TELEGRAM_BOT_TOKEN environment variable is required")
BOT_TOKEN = bot_token

target_channel = os.getenv('TARGET_CHANNEL')
if not target_channel:
    raise ValueError("TARGET_CHANNEL environment variable is required")
TARGET_CHANNEL = target_channel

openai_api_key = os.getenv('OPENAI_API_KEY')
if not openai_api_key:
    raise ValueError("OPENAI_API_KEY environment variable is required")
OPENAI_API_KEY = openai_api_key

source_channels_str = os.getenv('SOURCE_CHANNELS', '')
SOURCE_CHANNELS = set([c.strip() for c in source_channels_str.split(',') if c.strip()])

# –ù–æ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –≥—Ä—É–ø–ø
source_groups_str = os.getenv('SOURCE_GROUPS', '')
SOURCE_GROUPS = [g.strip() for g in source_groups_str.split(',') if g.strip()]

openai_client = OpenAI(api_key=OPENAI_API_KEY)

LINK_REGEX = re.compile(r"https?://\S+")

# Create separate clients for user (reading) and bot (sending)
user_client = TelegramClient('tg_summarizer_user', API_ID, API_HASH)
bot_client = TelegramClient('tg_summarizer_bot', API_ID, API_HASH)

SIMILARITY_THRESHOLD = 0.9
HISTORY_FILE = 'summarization_history.json'
SUMMARIES_HISTORY_FILE = 'summaries_history.json'
DISCOVERED_CHANNELS_FILE = 'discovered_channels.json'

# –ù–æ–≤—ã–µ —Ñ–∞–π–ª—ã –¥–ª—è –≥—Ä—É–ø–ø
GROUP_HISTORY_FILE = 'group_summarization_history.json'
GROUP_SUMMARIES_HISTORY_FILE = 'group_summaries_history.json'
GROUP_LAST_RUN_FILE = 'group_last_run.json'

# –§–ª–∞–≥ –¥–ª—è –æ—Ç–∫–ª—é—á–µ–Ω–∏—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ–∫—Ä—ã—Ç–∏—è –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–∞–º–º–∞—Ä–∏
ENABLE_SUMMARIES_DEDUPLICATION = True


@dataclass
class MessageInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–æ–æ–±—â–µ–Ω–∏–∏ Telegram"""
    text: str
    channel: str
    message_id: int
    date: datetime
    link: str
    
    def get_telegram_link(self) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—Å—ã–ª–∫—É –Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ Telegram"""
        # –£–±–∏—Ä–∞–µ–º @ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–∞–Ω–∞–ª–∞ –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è —Å—Å—ã–ª–∫–∏
        channel_name = self.channel.lstrip('@')
        return f"https://t.me/{channel_name}/{self.message_id}"
    
    def to_dict(self) -> dict:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –æ–±—ä–µ–∫—Ç –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ JSON"""
        return {
            'text': self.text,
            'channel': self.channel,
            'message_id': self.message_id,
            'date': self.date.isoformat(),
            'link': self.link
        }
    
    @classmethod
    def from_dict(cls, data: dict) -> 'MessageInfo':
        """–°–æ–∑–¥–∞–µ—Ç –æ–±—ä–µ–∫—Ç –∏–∑ —Å–ª–æ–≤–∞—Ä—è"""
        return cls(
            text=data['text'],
            channel=data['channel'],
            message_id=data['message_id'],
            date=datetime.fromisoformat(data['date']),
            link=data['link']
        )


@dataclass
class SummaryInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–æ–∑–¥–∞–Ω–Ω–æ–º —Å–∞–º–º–∞—Ä–∏"""
    content: str
    date: datetime
    message_count: int
    channels: List[str]
    
    def to_dict(self) -> dict:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –æ–±—ä–µ–∫—Ç –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ JSON"""
        return {
            'content': self.content,
            'date': self.date.isoformat(),
            'message_count': self.message_count,
            'channels': self.channels
        }
    
    @classmethod
    def from_dict(cls, data: dict) -> 'SummaryInfo':
        """–°–æ–∑–¥–∞–µ—Ç –æ–±—ä–µ–∫—Ç –∏–∑ —Å–ª–æ–≤–∞—Ä—è"""
        return cls(
            content=data['content'],
            date=datetime.fromisoformat(data['date']),
            message_count=data['message_count'],
            channels=data['channels']
        )


def count_characters(text: str) -> int:
    """–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ, –∏—Å–∫–ª—é—á–∞—è HTML-—Ç–µ–≥–∏."""
    # –£–¥–∞–ª—è–µ–º HTML-—Ç–µ–≥–∏ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –ø–æ–¥—Å—á–µ—Ç–∞ —Å–∏–º–≤–æ–ª–æ–≤
    import re
    clean_text = re.sub(r'<[^>]+>', '', text)
    return len(clean_text)


async def call_openai(system_prompt: str, user_content: str, max_tokens: int = 300) -> str:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–∑–æ–≤–∞ OpenAI API."""
    try:
        response = openai_client.chat.completions.create(
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ],
            model="gpt-4o-mini",
            max_tokens=max_tokens,
        )
        result = response.choices[0].message.content
        if result is None:
            return ""
        return result.strip()
    except Exception as e:
        print(f"OpenAI API error: {e}")
        return ""


def extract_links(text: str) -> list[str]:
    """Return all URLs from a string."""
    return LINK_REGEX.findall(text)


def load_channel_abbreviations() -> dict:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –∫–∞–Ω–∞–ª–æ–≤ –∏–∑ JSON —Ñ–∞–π–ª–∞."""
    abbreviations_file = 'channel_abbreviations.json'
    if not os.path.exists(abbreviations_file):
        return {}
    
    try:
        with open(abbreviations_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get('channel_abbreviations', {})
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä –∫–∞–Ω–∞–ª–æ–≤: {e}")
        return {}


def save_channel_abbreviation(channel_name: str, abbreviation: str) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–æ–≤—É—é –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—É –∫–∞–Ω–∞–ª–∞ –≤ JSON —Ñ–∞–π–ª."""
    abbreviations_file = 'channel_abbreviations.json'
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
        if os.path.exists(abbreviations_file):
            with open(abbreviations_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
        else:
            data = {'channel_abbreviations': {}, 'last_updated': ''}
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—É
        data['channel_abbreviations'][channel_name] = abbreviation
        data['last_updated'] = datetime.now().isoformat()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        with open(abbreviations_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
            
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –∫–∞–Ω–∞–ª–∞: {e}")


def create_channel_abbreviation(channel_name: str) -> str:
    """–°–æ–∑–¥–∞–µ—Ç –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—É –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–∞–Ω–∞–ª–∞."""
    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
    existing_abbreviations = load_channel_abbreviations()
    
    if channel_name in existing_abbreviations:
        return existing_abbreviations[channel_name]
    
    # –ï—Å–ª–∏ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –Ω–µ—Ç, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é
    clean_name = channel_name.lstrip('@')
    words = re.split(r'[\s\-_]+', clean_name)
    abbreviation = ''.join(word[0].upper() for word in words if word)
    
    # –ï—Å–ª–∏ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–∞—è, –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3-4 –±—É–∫–≤—ã
    if len(abbreviation) > 4:
        abbreviation = abbreviation[:4]
    
    # –ï—Å–ª–∏ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞ –ø—É—Å—Ç–∞—è –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–µ –±—É–∫–≤—ã –Ω–∞–∑–≤–∞–Ω–∏—è
    if len(abbreviation) < 2:
        abbreviation = clean_name[:3].upper()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç—É–µ—Ç –ª–∏ –Ω–æ–≤–∞—è –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏
    existing_values = set(existing_abbreviations.values())
    if abbreviation in existing_values:
        # –ï—Å–ª–∏ –∫–æ–Ω—Ñ–ª–∏–∫—Ç, –¥–æ–±–∞–≤–ª—è–µ–º —Ü–∏—Ñ—Ä—É
        counter = 1
        while f"{abbreviation}{counter}" in existing_values:
            counter += 1
        abbreviation = f"{abbreviation}{counter}"
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—É—é –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—É
    save_channel_abbreviation(channel_name, abbreviation)
    
    return abbreviation


def load_summarization_history() -> Set[str]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ —Ñ–∞–π–ª–∞."""
    if not os.path.exists(HISTORY_FILE):
        return set()
    
    try:
        with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
            processed_messages = set()
            for msg_data in data.get('processed_messages', []):
                msg = MessageInfo.from_dict(msg_data)
                # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä: –∫–∞–Ω–∞–ª + message_id + —Ö–µ—à —Ç–µ–∫—Å—Ç–∞
                msg_id = f"{msg.channel}_{msg.message_id}_{hash(msg.text)}"
                processed_messages.add(msg_id)
            return processed_messages
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏—Å—Ç–æ—Ä–∏–∏: {e}")
        return set()


def save_summarization_history(messages: List[MessageInfo]) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –∏—Å—Ç–æ—Ä–∏—é."""
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∏—Å—Ç–æ—Ä–∏—é
        existing_data = []
        if os.path.exists(HISTORY_FILE):
            with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                existing_data = data.get('processed_messages', [])
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        new_messages = [msg.to_dict() for msg in messages]
        all_messages = existing_data + new_messages
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏ 1000 —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏
        if len(all_messages) > 1000:
            all_messages = all_messages[-1000:]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é
        with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
            json.dump({
                'processed_messages': all_messages,
                'last_updated': datetime.now().isoformat()
            }, f, ensure_ascii=False, indent=2)
            
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∏—Å—Ç–æ—Ä–∏–∏: {e}")


def load_summaries_history() -> List[SummaryInfo]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —Å–∞–º–º–∞—Ä–∏ –∏–∑ —Ñ–∞–π–ª–∞."""
    if not os.path.exists(SUMMARIES_HISTORY_FILE):
        return []
    
    try:
        with open(SUMMARIES_HISTORY_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            summaries = []
            for summary_data in data.get('summaries', []):
                summaries.append(SummaryInfo.from_dict(summary_data))
            return summaries
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏—Å—Ç–æ—Ä–∏–∏ —Å–∞–º–º–∞—Ä–∏: {e}")
        return []


def save_summary_to_history(summary: SummaryInfo) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–æ–≤–æ–µ —Å–∞–º–º–∞—Ä–∏ –≤ –∏—Å—Ç–æ—Ä–∏—é."""
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∏—Å—Ç–æ—Ä–∏—é
        existing_summaries = []
        if os.path.exists(SUMMARIES_HISTORY_FILE):
            with open(SUMMARIES_HISTORY_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                existing_summaries = data.get('summaries', [])
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤–æ–µ —Å–∞–º–º–∞—Ä–∏
        new_summary = summary.to_dict()
        all_summaries = existing_summaries + [new_summary]
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏ 50 —Å–∞–º–º–∞—Ä–∏
        if len(all_summaries) > 50:
            all_summaries = all_summaries[-50:]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é
        with open(SUMMARIES_HISTORY_FILE, 'w', encoding='utf-8') as f:
            json.dump({
                'summaries': all_summaries,
                'last_updated': datetime.now().isoformat()
            }, f, ensure_ascii=False, indent=2)
            
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∏—Å—Ç–æ—Ä–∏–∏ —Å–∞–º–º–∞—Ä–∏: {e}")


def load_discovered_channels() -> List[str]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞."""
    if not os.path.exists(DISCOVERED_CHANNELS_FILE):
        return []
    
    try:
        with open(DISCOVERED_CHANNELS_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get('discovered_channels', [])
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤: {e}")
        return []


def load_similar_channels() -> List[str]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø–æ—Ö–æ–∂–∏—Ö –∫–∞–Ω–∞–ª–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞."""
    if not os.path.exists(DISCOVERED_CHANNELS_FILE):
        return []
    
    try:
        with open(DISCOVERED_CHANNELS_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get('similar_channels', [])
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –ø–æ—Ö–æ–∂–∏—Ö –∫–∞–Ω–∞–ª–æ–≤: {e}")
        return []


def load_banned_channels() -> List[str]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞."""
    if not os.path.exists(DISCOVERED_CHANNELS_FILE):
        return []
    
    try:
        with open(DISCOVERED_CHANNELS_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get('banned_channels', [])
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤: {e}")
        return []


def save_discovered_channel(channel_name: str) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–æ–≤—ã–π –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–π –∫–∞–Ω–∞–ª –≤ JSON —Ñ–∞–π–ª."""
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
        if os.path.exists(DISCOVERED_CHANNELS_FILE):
            with open(DISCOVERED_CHANNELS_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
        else:
            data = {'discovered_channels': [], 'similar_channels': [], 'last_updated': ''}
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ—Ç –ª–∏ —É–∂–µ —Ç–∞–∫–æ–≥–æ –∫–∞–Ω–∞–ª–∞
        if channel_name not in data['discovered_channels']:
            data['discovered_channels'].append(channel_name)
            data['last_updated'] = datetime.now().isoformat()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            with open(DISCOVERED_CHANNELS_FILE, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
                
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω–æ–≥–æ –∫–∞–Ω–∞–ª–∞: {e}")


def save_similar_channel(channel_name: str) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ—Ö–æ–∂–∏–π –∫–∞–Ω–∞–ª –≤ JSON —Ñ–∞–π–ª."""
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
        if os.path.exists(DISCOVERED_CHANNELS_FILE):
            with open(DISCOVERED_CHANNELS_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
        else:
            data = {'discovered_channels': [], 'similar_channels': [], 'banned_channels': [], 'last_updated': ''}
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ—Ç –ª–∏ —É–∂–µ —Ç–∞–∫–æ–≥–æ –∫–∞–Ω–∞–ª–∞
        if channel_name not in data['similar_channels']:
            data['similar_channels'].append(channel_name)
            data['last_updated'] = datetime.now().isoformat()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            with open(DISCOVERED_CHANNELS_FILE, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
                
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –ø–æ—Ö–æ–∂–µ–≥–æ –∫–∞–Ω–∞–ª–∞: {e}")


def save_banned_channel(channel_name: str) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–æ–≤—ã–π –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–∞–Ω–∞–ª –≤ JSON —Ñ–∞–π–ª."""
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
        if os.path.exists(DISCOVERED_CHANNELS_FILE):
            with open(DISCOVERED_CHANNELS_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
        else:
            data = {'discovered_channels': [], 'similar_channels': [], 'banned_channels': [], 'last_updated': ''}
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ—Ç –ª–∏ —É–∂–µ —Ç–∞–∫–æ–≥–æ –∫–∞–Ω–∞–ª–∞
        if channel_name not in data['banned_channels']:
            data['banned_channels'].append(channel_name)
            data['last_updated'] = datetime.now().isoformat()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            with open(DISCOVERED_CHANNELS_FILE, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
                
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–∞–Ω–∞–ª–∞: {e}")


async def get_similar_channels_from_telegram(channel_username: str = None) -> List[str]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø–æ—Ö–æ–∂–∏—Ö –∫–∞–Ω–∞–ª–æ–≤ —á–µ—Ä–µ–∑ Telegram API."""
    try:
        if not user_client.is_connected():
            await user_client.start()
        
        # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω –∫–∞–Ω–∞–ª, –ø–æ–ª—É—á–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –Ω–µ–≥–æ
        if channel_username:
            # –£–±–∏—Ä–∞–µ–º @ –µ—Å–ª–∏ –µ—Å—Ç—å
            channel_username = channel_username.lstrip('@')
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–∞–Ω–∞–ª–µ
            try:
                channel_entity = await user_client.get_entity(f"@{channel_username}")
                channel_input = InputChannel(channel_entity.id, channel_entity.access_hash)
                
                # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
                result = await user_client(GetChannelRecommendationsRequest(
                    channel=channel_input
                ))
                
                similar_channels = []
                for chat in result.chats:
                    if hasattr(chat, 'username') and chat.username:
                        similar_channels.append(f"@{chat.username}")
                
                return similar_channels
                
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è –∫–∞–Ω–∞–ª–∞ {channel_username}: {e}")
                return []
        
        # –ï—Å–ª–∏ –∫–∞–Ω–∞–ª –Ω–µ —É–∫–∞–∑–∞–Ω, –ø–æ–ª—É—á–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        else:
            result = await user_client(GetChannelRecommendationsRequest())
            
            similar_channels = []
            for chat in result.chats:
                if hasattr(chat, 'username') and chat.username:
                    similar_channels.append(f"@{chat.username}")
            
            return similar_channels
            
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ø–æ—Ö–æ–∂–∏—Ö –∫–∞–Ω–∞–ª–æ–≤: {e}")
        return []


async def discover_and_save_similar_channels(channel_username: str = None) -> None:
    """–û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –∫–∞–Ω–∞–ª—ã."""
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–∞–Ω–∞–ª—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        existing_discovered = set(load_discovered_channels())
        existing_similar = set(load_similar_channels())
        existing_banned = set(load_banned_channels())
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–∞–Ω–∞–ª—ã
        existing_channels = SOURCE_CHANNELS | existing_discovered | existing_similar | existing_banned
        
        if channel_username:
            # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –∫–∞–Ω–∞–ª, –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –æ–Ω –≤ SOURCE_CHANNELS
            if channel_username not in SOURCE_CHANNELS:
                print(f"–ö–∞–Ω–∞–ª {channel_username} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ SOURCE_CHANNELS. "
                      f"–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –∫–∞–Ω–∞–ª–æ–≤ —Ç–æ–ª—å–∫–æ –¥–ª—è –∫–∞–Ω–∞–ª–æ–≤ –∏–∑ SOURCE_CHANNELS.")
                return
            
            print(f"–ü–æ–ª—É—á–∞–µ–º –ø–æ—Ö–æ–∂–∏–µ –∫–∞–Ω–∞–ª—ã –¥–ª—è {channel_username}...")
            similar_channels = await get_similar_channels_from_telegram(channel_username)
        else:
            # –ï—Å–ª–∏ –∫–∞–Ω–∞–ª –Ω–µ —É–∫–∞–∑–∞–Ω, –∏—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –¥–ª—è –≤—Å–µ—Ö –∫–∞–Ω–∞–ª–æ–≤ –∏–∑ SOURCE_CHANNELS
            print(f"–ü–æ–ª—É—á–∞–µ–º –ø–æ—Ö–æ–∂–∏–µ –∫–∞–Ω–∞–ª—ã –¥–ª—è –≤—Å–µ—Ö –∫–∞–Ω–∞–ª–æ–≤ –∏–∑ SOURCE_CHANNELS...")
            all_similar_channels = set()
            
            for source_channel in SOURCE_CHANNELS:
                print(f"  –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –¥–ª—è {source_channel}...")
                similar_channels = await get_similar_channels_from_telegram(source_channel)
                all_similar_channels.update(similar_channels)
                # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                await asyncio.sleep(1)
            
            similar_channels = list(all_similar_channels)
        
        if similar_channels:
            # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–∞–Ω–∞–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
            new_similar_channels = []
            skipped_channels = []
            
            for channel in similar_channels:
                if channel in existing_channels:
                    skipped_channels.append(channel)
                else:
                    new_similar_channels.append(channel)
            
            if new_similar_channels:
                print(f"–ù–∞–π–¥–µ–Ω–æ {len(new_similar_channels)} –Ω–æ–≤—ã—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ—Ö–æ–∂–∏—Ö –∫–∞–Ω–∞–ª–æ–≤:")
                for channel in new_similar_channels:
                    print(f"  - {channel}")
                    save_similar_channel(channel)
                print("–ù–æ–≤—ã–µ –ø–æ—Ö–æ–∂–∏–µ –∫–∞–Ω–∞–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ discovered_channels.json")
            else:
                print("–í—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∫–∞–Ω–∞–ª—ã —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç –≤ —Å–∏—Å—Ç–µ–º–µ")
            
            if skipped_channels:
                print(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ {len(skipped_channels)} –∫–∞–Ω–∞–ª–æ–≤ (—É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç):")
                for channel in skipped_channels[:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 10
                    print(f"  - {channel}")
                if len(skipped_channels) > 10:
                    print(f"  ... –∏ –µ—â–µ {len(skipped_channels) - 10} –∫–∞–Ω–∞–ª–æ–≤")
        else:
            print("–ü–æ—Ö–æ–∂–∏—Ö –∫–∞–Ω–∞–ª–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ –ø–æ—Ö–æ–∂–∏—Ö –∫–∞–Ω–∞–ª–æ–≤: {e}")


def load_group_summarization_history() -> Set[str]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ –≥—Ä—É–ø–ø –∏–∑ —Ñ–∞–π–ª–∞."""
    if not os.path.exists(GROUP_HISTORY_FILE):
        return set()
    
    try:
        with open(GROUP_HISTORY_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
            processed_messages = set()
            for msg_data in data.get('processed_messages', []):
                msg = MessageInfo.from_dict(msg_data)
                # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä: –∫–∞–Ω–∞–ª + message_id + —Ö–µ—à —Ç–µ–∫—Å—Ç–∞
                msg_id = f"{msg.channel}_{msg.message_id}_{hash(msg.text)}"
                processed_messages.add(msg_id)
            return processed_messages
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏—Å—Ç–æ—Ä–∏–∏ –≥—Ä—É–ø–ø: {e}")
        return set()


def save_group_summarization_history(messages: List[MessageInfo]) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ –≥—Ä—É–ø–ø –≤ —Ñ–∞–π–ª."""
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
        if os.path.exists(GROUP_HISTORY_FILE):
            with open(GROUP_HISTORY_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
        else:
            data = {'processed_messages': [], 'last_updated': ''}
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        for msg in messages:
            data['processed_messages'].append(msg.to_dict())
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏ 1000 —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏
        if len(data['processed_messages']) > 1000:
            data['processed_messages'] = data['processed_messages'][-1000:]
        
        data['last_updated'] = datetime.now().isoformat()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        with open(GROUP_HISTORY_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
            
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∏—Å—Ç–æ—Ä–∏–∏ –≥—Ä—É–ø–ø: {e}")


def load_group_summaries_history() -> List[SummaryInfo]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —Å–∞–º–º–∞—Ä–∏ –∏–∑ –≥—Ä—É–ø–ø –∏–∑ —Ñ–∞–π–ª–∞."""
    if not os.path.exists(GROUP_SUMMARIES_HISTORY_FILE):
        return []
    
    try:
        with open(GROUP_SUMMARIES_HISTORY_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            summaries = []
            for summary_data in data.get('summaries', []):
                summaries.append(SummaryInfo.from_dict(summary_data))
            return summaries
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏—Å—Ç–æ—Ä–∏–∏ —Å–∞–º–º–∞—Ä–∏ –≥—Ä—É–ø–ø: {e}")
        return []


def save_group_summary_to_history(summary: SummaryInfo) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–∞–º–º–∞—Ä–∏ –∏–∑ –≥—Ä—É–ø–ø –≤ –∏—Å—Ç–æ—Ä–∏—é."""
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
        if os.path.exists(GROUP_SUMMARIES_HISTORY_FILE):
            with open(GROUP_SUMMARIES_HISTORY_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
        else:
            data = {'summaries': [], 'last_updated': ''}
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤–æ–µ —Å–∞–º–º–∞—Ä–∏
        data['summaries'].append(summary.to_dict())
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏ 100 —Å–∞–º–º–∞—Ä–∏
        if len(data['summaries']) > 100:
            data['summaries'] = data['summaries'][-100:]
        
        data['last_updated'] = datetime.now().isoformat()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        with open(GROUP_SUMMARIES_HISTORY_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
            
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Å–∞–º–º–∞—Ä–∏ –≥—Ä—É–ø–ø –≤ –∏—Å—Ç–æ—Ä–∏—é: {e}")


def should_run_group_summarization() -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –∑–∞–ø—É—Å–∫–∞—Ç—å —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é –≥—Ä—É–ø–ø (—Ä–∞–∑ –≤ —Å—É—Ç–∫–∏)."""
    if not os.path.exists(GROUP_LAST_RUN_FILE):
        return True
    
    try:
        with open(GROUP_LAST_RUN_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            last_run_str = data.get('last_run', '')
            if not last_run_str:
                return True
            
            last_run = datetime.fromisoformat(last_run_str)
            now = datetime.now(timezone.utc)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø—Ä–æ—à–ª–æ –ª–∏ –±–æ–ª—å—à–µ 24 —á–∞—Å–æ–≤ —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–∞–ø—É—Å–∫–∞
            return (now - last_run).total_seconds() > 24 * 60 * 60
            
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–∞–ø—É—Å–∫–∞ –≥—Ä—É–ø–ø: {e}")
        return True


def update_group_last_run() -> None:
    """–û–±–Ω–æ–≤–ª—è–µ—Ç –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–∞–ø—É—Å–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –≥—Ä—É–ø–ø."""
    try:
        data = {
            'last_run': datetime.now(timezone.utc).isoformat(),
            'last_updated': datetime.now().isoformat()
        }
        
        with open(GROUP_LAST_RUN_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
            
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–∞–ø—É—Å–∫–∞ –≥—Ä—É–ø–ø: {e}")


def get_recent_group_summaries_context(days: int = 7) -> str:
    """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–∞–º–º–∞—Ä–∏ –∏–∑ –≥—Ä—É–ø–ø –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏."""
    summaries = load_group_summaries_history()
    if not summaries:
        return ""
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–∞–º–º–∞—Ä–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –¥–Ω–µ–π
    cutoff_date = datetime.now(timezone.utc) - timedelta(days=days)
    recent_summaries = [s for s in summaries if s.date >= cutoff_date]
    
    if not recent_summaries:
        return ""
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å–∞–º–º–∞—Ä–∏
    context_parts = []
    for summary in recent_summaries:
        context_parts.append(f"–î–∞—Ç–∞: {summary.date.strftime('%Y-%m-%d')}")
        context_parts.append(f"–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {summary.content}")
        context_parts.append("---")
    
    return "\n".join(context_parts)


def get_all_source_channels() -> List[str]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∫–∞–Ω–∞–ª–æ–≤ –∏–∑ .env, –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö –∏ –ø–æ—Ö–æ–∂–∏—Ö –∫–∞–Ω–∞–ª–æ–≤."""
    discovered_channels = set(load_discovered_channels())
    similar_channels = set(load_similar_channels())
    banned_channels = set(load_banned_channels())
    
    # –ò—Å–∫–ª—é—á–∞–µ–º –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–∞–Ω–∞–ª—ã
    all_channels_set = SOURCE_CHANNELS | discovered_channels | similar_channels
    all_channels_set = all_channels_set - banned_channels
    
    all_channels = list(all_channels_set)
    random.shuffle(all_channels)
    print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º {len(SOURCE_CHANNELS)} –∫–∞–Ω–∞–ª–æ–≤ –∏–∑ .env, "
          f"{len(discovered_channels)} –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö, {len(similar_channels)} –ø–æ—Ö–æ–∂–∏—Ö –∫–∞–Ω–∞–ª–æ–≤ "
          f"(–∏—Å–∫–ª—é—á–µ–Ω–æ {len(banned_channels)} –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö)")
    return all_channels


def get_recent_summaries_context(days: int = 3) -> str:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–∞–º–º–∞—Ä–∏ –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏."""
    summaries = load_summaries_history()
    if not summaries:
        return ""
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–∞–º–º–∞—Ä–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –¥–Ω–µ–π
    cutoff_date = datetime.now(timezone.utc) - timedelta(days=days)
    recent_summaries = [s for s in summaries if s.date >= cutoff_date]
    
    if not recent_summaries:
        return ""
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 50 —Å–∞–º–º–∞—Ä–∏
    context_parts = []
    for i, summary in enumerate(recent_summaries[-50:], 1):
        # –û—á–∏—â–∞–µ–º HTML —Ç–µ–≥–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        clean_content = re.sub(r'<[^>]+>', '', summary.content)
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        context_parts.append(f"–°–∞–º–º–∞—Ä–∏ {i} ({summary.date.strftime('%Y-%m-%d')}):\n{clean_content[:500]}...")
    
    return "\n\n".join(context_parts)


def is_message_processed(msg: MessageInfo, processed_messages: Set[str]) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –±—ã–ª–æ –ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ä–∞–Ω–µ–µ."""
    msg_id = f"{msg.channel}_{msg.message_id}_{hash(msg.text)}"
    return msg_id in processed_messages


async def are_messages_duplicate(msg_a: MessageInfo, msg_b: MessageInfo) -> bool:
    """Use the LLM to see if two messages cover the same topic."""
    system_prompt = (
        "–û–ø–∏—Å—ã–≤–∞—é—Ç –ª–∏ —Å–ª–µ–¥—É—é—â–∏–µ –¥–≤–∞ —Å–æ–æ–±—â–µ–Ω–∏—è Telegram –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –∏–ª–∏ —Å—Ç–∞—Ç—å—é?\n"
        "–û—Ç–≤–µ—Ç—å—Ç–µ –¥–∞ –∏–ª–∏ –Ω–µ—Ç."
    )
    user_content = f"Message 1:\n{msg_a.text}\n\nMessage 2:\n{msg_b.text}"
    
    answer = await call_openai(system_prompt, user_content, max_tokens=1)
    return answer.lower().startswith("y")


async def is_message_covered_in_summaries(msg: MessageInfo) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –±—ã–ª–∞ –ª–∏ —Ç–µ–º–∞ —Å–æ–æ–±—â–µ–Ω–∏—è —É–∂–µ –æ—Å–≤–µ—â–µ–Ω–∞ –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–∞–º–º–∞—Ä–∏."""
    if not ENABLE_SUMMARIES_DEDUPLICATION:
        return False
    
    recent_summaries = get_recent_summaries_context()
    if not recent_summaries:
        return False
    
    system_prompt = """–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —Ç–µ–∫—Å—Ç–æ–≤. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –±—ã–ª–∞ –ª–∏ —Ç–µ–º–∞ –∏–∑ –Ω–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è —É–∂–µ –æ—Å–≤–µ—â–µ–Ω–∞ –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –¥–∞–π–¥–∂–µ—Å—Ç–∞—Ö.

    –ü—Ä–∞–≤–∏–ª–∞:
    1. –°—Ä–∞–≤–Ω–∏–≤–∞–π –¢–ï–ú–£ –∏ –û–°–ù–û–í–ù–£–Æ –ò–î–ï–Æ —Å–æ–æ–±—â–µ–Ω–∏—è, –∞ –Ω–µ —Ç–æ—á–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏
    2. –ï—Å–ª–∏ —Ç–µ–º–∞ —É–∂–µ –±—ã–ª–∞ –æ—Å–≤–µ—â–µ–Ω–∞ –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –¥–∞–π–¥–∂–µ—Å—Ç–∞—Ö, –æ—Ç–≤–µ—á–∞–π "–î–ê"
    3. –ï—Å–ª–∏ —ç—Ç–æ –Ω–æ–≤–∞—è —Ç–µ–º–∞, –æ—Ç–≤–µ—á–∞–π "–ù–ï–¢"
    4. –£—á–∏—Ç—ã–≤–∞–π —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∏–≥–Ω–æ—Ä–∏—Ä—É–π –º–µ–ª–∫–∏–µ –¥–µ—Ç–∞–ª–∏

    –ü—Ä–∏–º–µ—Ä—ã:
    - "–ù–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º GPT-5" –∏ "OpenAI –∞–Ω–æ–Ω—Å–∏—Ä–æ–≤–∞–ª GPT-5" = –î–ê (—Ç–∞ –∂–µ —Ç–µ–º–∞)
    - "–ù–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º GPT-5" –∏ "–ù–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º BERT" = –ù–ï–¢ (—Ä–∞–∑–Ω—ã–µ —Ç–µ–º—ã)
    - "–¶–µ–Ω–∞ –∞–∫—Ü–∏–π NVIDIA –≤—ã—Ä–æ—Å–ª–∞" –∏ "NVIDIA –ø–æ–∫–∞–∑–∞–ª–∞ —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã" = –î–ê (—Ç–∞ –∂–µ —Ç–µ–º–∞)

    –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ "–î–ê" –∏–ª–∏ "–ù–ï–¢"."""

    user_content = f"""–ü—Ä–µ–¥—ã–¥—É—â–∏–µ –¥–∞–π–¥–∂–µ—Å—Ç—ã:
        {recent_summaries}

        –ù–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ:
        {msg.text}

        –ë—ã–ª–∞ –ª–∏ —ç—Ç–∞ —Ç–µ–º–∞ —É–∂–µ –æ—Å–≤–µ—â–µ–Ω–∞ –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –¥–∞–π–¥–∂–µ—Å—Ç–∞—Ö?"""

    try:
        result = await call_openai(system_prompt, user_content, max_tokens=10)
        return result.strip().upper() == "–î–ê"
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –ø–æ–∫—Ä—ã—Ç–∏—è –≤ —Å–∞–º–º–∞—Ä–∏: {e}")
        return False


async def is_message_covered_in_group_summaries(msg: MessageInfo) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –±—ã–ª–∞ –ª–∏ —Ç–µ–º–∞ —Å–æ–æ–±—â–µ–Ω–∏—è —É–∂–µ –æ—Å–≤–µ—â–µ–Ω–∞ –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–∞–º–º–∞—Ä–∏ –≥—Ä—É–ø–ø."""
    if not ENABLE_SUMMARIES_DEDUPLICATION:
        return False
    
    recent_summaries = get_recent_group_summaries_context()
    if not recent_summaries:
        return False
    
    system_prompt = """–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —Ç–µ–∫—Å—Ç–æ–≤. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –±—ã–ª–∞ –ª–∏ —Ç–µ–º–∞ –∏–∑ –Ω–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è —É–∂–µ –æ—Å–≤–µ—â–µ–Ω–∞ –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –¥–∞–π–¥–∂–µ—Å—Ç–∞—Ö –≥—Ä—É–ø–ø.

–ü—Ä–∞–≤–∏–ª–∞:
1. –°—Ä–∞–≤–Ω–∏–≤–∞–π –¢–ï–ú–£ –∏ –û–°–ù–û–í–ù–£–Æ –ò–î–ï–Æ —Å–æ–æ–±—â–µ–Ω–∏—è, –∞ –Ω–µ —Ç–æ—á–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏
2. –ï—Å–ª–∏ —Ç–µ–º–∞ —É–∂–µ –±—ã–ª–∞ –æ—Å–≤–µ—â–µ–Ω–∞ –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –¥–∞–π–¥–∂–µ—Å—Ç–∞—Ö –≥—Ä—É–ø–ø, –æ—Ç–≤–µ—á–∞–π "–î–ê"
3. –ï—Å–ª–∏ —ç—Ç–æ –Ω–æ–≤–∞—è —Ç–µ–º–∞, –æ—Ç–≤–µ—á–∞–π "–ù–ï–¢"
4. –£—á–∏—Ç—ã–≤–∞–π —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∏–≥–Ω–æ—Ä–∏—Ä—É–π –º–µ–ª–∫–∏–µ –¥–µ—Ç–∞–ª–∏

–ü—Ä–∏–º–µ—Ä—ã:
- "–ù–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º GPT-5" –∏ "OpenAI –∞–Ω–æ–Ω—Å–∏—Ä–æ–≤–∞–ª GPT-5" = –î–ê (—Ç–∞ –∂–µ —Ç–µ–º–∞)
- "–ù–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º GPT-5" –∏ "–ù–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º BERT" = –ù–ï–¢ (—Ä–∞–∑–Ω—ã–µ —Ç–µ–º—ã)
- "–¶–µ–Ω–∞ –∞–∫—Ü–∏–π NVIDIA –≤—ã—Ä–æ—Å–ª–∞" –∏ "NVIDIA –ø–æ–∫–∞–∑–∞–ª–∞ —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã" = –î–ê (—Ç–∞ –∂–µ —Ç–µ–º–∞)

–û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ "–î–ê" –∏–ª–∏ "–ù–ï–¢"."""

    user_content = f"""–ü—Ä–µ–¥—ã–¥—É—â–∏–µ –¥–∞–π–¥–∂–µ—Å—Ç—ã –≥—Ä—É–ø–ø:
{recent_summaries}

–ù–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ:
{msg.text}

–ë—ã–ª–∞ –ª–∏ —ç—Ç–∞ —Ç–µ–º–∞ —É–∂–µ –æ—Å–≤–µ—â–µ–Ω–∞ –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –¥–∞–π–¥–∂–µ—Å—Ç–∞—Ö –≥—Ä—É–ø–ø?"""

    try:
        result = await call_openai(system_prompt, user_content, max_tokens=10)
        return result.strip().upper() == "–î–ê"
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –ø–æ–∫—Ä—ã—Ç–∏—è –≤ —Å–∞–º–º–∞—Ä–∏ –≥—Ä—É–ø–ø: {e}")
        return False


async def is_nlp_related(text: str) -> bool:
    """Use the LLM to decide if a message is NLP related and not advertising."""
    system_prompt = (
        "–í—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä. –û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º –¥–ª—è NLP/ML/AI/programming/Python –¥–∞–π–¥–∂–µ—Å—Ç–∞.\n\n"
        "–ü–†–ò–ù–ò–ú–ê–ô–¢–ï (–æ—Ç–≤–µ—Ç—å—Ç–µ '–¥–∞'):\n"
        "- –ù–∞—É—á–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è\n"
        "- –ù–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –±–∏–±–ª–∏–æ—Ç–µ–∫–∏, –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã\n"
        "- –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –æ–±–∑–æ—Ä—ã –∏ –±–µ–Ω—á–º–∞—Ä–∫–∏\n"
        "- –ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏ –∏ –≤–æ—Ä–∫—à–æ–ø—ã\n"
        "- –û—Ç–∫—Ä—ã—Ç—ã–µ –ø—Ä–æ–µ–∫—Ç—ã –∏ –¥–∞—Ç–∞—Å–µ—Ç—ã\n"
        "- –ù–æ–≤–æ—Å—Ç–∏ –ø—Ä–æ Kaggle, —Ö–∞–∫–∞—Ç–æ–Ω—ã –∏ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è\n"
        "- –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏ —Å–æ–≤–µ—Ç—ã –æ –∫–∞—Ä—å–µ—Ä–µ –∏ –≤–∞–∫–∞–Ω—Å–∏—è—Ö\n"
        "- –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–µ—Ç-–ø—Ä–æ–µ–∫—Ç–∞—Ö\n"
        "- –¢–µ–∫—Å—Ç—ã –ø–æ –∏–∏ –∏ bigtech –∫–æ–º–ø–∞–Ω–∏–∏\n"
        "- –¢–µ–∫—Å—Ç—ã –ø—Ä–æ —Å—Ç–∞—Ä—Ç–∞–ø—ã\n"
        "- –¢–µ–∫—Å—Ç—ã –ø—Ä–æ –∏–∏-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ –∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è (ChatGPT, Claude, Gemini, Grok, DeepSeek, KlingAi, Midjourney etc.)\n"
        "- –õ—é–±—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –ø—Ä–æ –°—ç–º–∞ –ê–ª—å—Ç–º–∞–Ω–∞ (Sam Altman), –ú–∞—Ä–∫–∞ –¶—É–∫–µ—Ä–±–µ—Ä–≥–∞"
        "OpenAI, Anthropic, Google, Meta, Microsoft, Nvidia, FAANG –∏ —Ç.–¥.\n"
        "- –ù–æ–≤–æ—Å—Ç–∏ –ø—Ä–æ –ø–æ–∫—É–ø–∫—É, –ø—Ä–æ–¥–∞–∂—É –∫–æ–º–ø–∞–Ω–∏–π –∏ –ø–æ–≥–ª–æ—â–µ–Ω–∏—è\n:"
        "- –ù–æ–≤–æ—Å—Ç–∏ –ø—Ä–æ —É–≤–æ–ª—å–Ω–µ–Ω–∏—è –∏ –ò–ò-—ç–∫–æ–Ω–æ–º–∏–∫—É\n:"
        "- –ù–æ–≤–æ—Å—Ç–∏ –ø—Ä–æ GPU –∏ –∂–µ–ª–µ–∑–æ\n:"
        "- –ù–æ–≤–æ—Å—Ç–∏ –ø—Ä–æ –≤–∞–π–±–∫–æ–¥–∏–Ω–≥ (vibe coding) –∏ –∏–∏-–∞–≥–µ–Ω—Ç–æ–≤ (—á–∞—Å—Ç–æ –ø—Ä–æ—Å—Ç–æ –∞–≥–µ–Ω—Ç—ã, agents)\n:"
        "- –ù–æ–≤–æ—Å—Ç–∏ –ø—Ä–æ LLM –∏ –∏—Ö –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ\n:"
        "- –ù–æ–≤–æ—Å—Ç–∏ –ø—Ä–æ –æ–ø—ã—Ç —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è\n:"
        "- –ú–µ–º—ã –∏–∑ –º–∏—Ä–∞ –∏–∏ –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π\n:"
        "- –ü—Ä–æ–º–ø—Ç-–∏–Ω–∂–µ–Ω–∏—Ä–∏–Ω–≥\n:"

        "–û–¢–ö–õ–û–ù–Ø–ô–¢–ï (–æ—Ç–≤–µ—Ç—å—Ç–µ '–Ω–µ—Ç'):\n"
        "- –ö—É—Ä—Å—ã, –æ–±—É—á–µ–Ω–∏–µ, –ø–ª–∞—Ç–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã\n"
        "- –†–µ–∫–ª–∞–º–∞ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö LLM (GigaChat, YandexGPT)\n"
        "- –ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ —É—Å–ª—É–≥–∏\n"
        "- –í–µ–±–∏–Ω–∞—Ä—ã —Å –ø—Ä–æ–¥–∞–∂–∞–º–∏\n"
        "- –ú–∞—Å—Ç–µ—Ä-–∫–ª–∞—Å—Å—ã —Å —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–∞–º–∏\n"
        "- Hiring days\n"
        "- –í —Å–æ–æ–±—â–µ–Ω–∏–∏ –µ—Å—Ç—å —Å—Å—ã–ª–∫–∏ –≤–µ–¥—É—â–∏–µ –Ω–∞ –±–æ—Ç–æ–≤, –ø—Ä–∏ —ç—Ç–æ–º –Ω–µ —É–∫–∞–∑–∞–Ω–æ –ø–µ—Ä–µ–¥ —Å—Å—ã–ª–∫–æ–π, —á—Ç–æ —ç—Ç–æ —Å—Å—ã–ª–∫–∞ –Ω–∞ –±–æ—Ç–∞\n"
        "- –°–æ–æ–±—â–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å—Å—ã–ª–∫—É –Ω–∞ –±–æ—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, @ChattyEnglishBot)\n"
        "–û—Ç–≤–µ—á–∞–π—Ç–µ —Ç–æ–ª—å–∫–æ '–¥–∞' –∏–ª–∏ '–Ω–µ—Ç'."
    )
    
    answer = await call_openai(system_prompt, text, max_tokens=5)
    return answer.lower().strip().startswith('–¥–∞')


async def summarize_text(messages: List[MessageInfo]) -> str:
    """Call LLM to summarize the given messages with links."""
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –Ω–æ–º–µ—Ä–æ–≤ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    messages_with_sources = []
    total_original_length = 0
    
    for i, msg in enumerate(messages, 1):
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å–æ–æ–±—â–µ–Ω–∏—è
        links = extract_links(msg.text)
        source_info = f"[{i}] {msg.text}"
        if links:
            source_info += f" (–°—Å—ã–ª–∫–∏: {', '.join(links)})"
        messages_with_sources.append(source_info)
        total_original_length += count_characters(msg.text)
    
    messages_text = "\n\n".join(messages_with_sources)
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É —Å–∞–º–º–∞—Ä–∏
    max_summary_length = min(total_original_length * 2, 16000)
    
    system_prompt = (
        "–û–±–æ–±—â–∏—Ç–µ —Å–ª–µ–¥—É—é—â–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è Telegram –≤ –∫—Ä–∞—Ç–∫–∏–π –µ–∂–µ–¥–Ω–µ–≤–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç, —Å—Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ NLP. "
        "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π—Ç–µ –¥–∞–π–¥–∂–µ—Å—Ç –ø–æ —Ç–µ–º–∞–º –∏–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –±–ª–æ–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —É–∫–∞–∑—ã–≤–∞–π—Ç–µ "
        "–Ω–æ–º–µ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –≤ –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã—Ö —Å–∫–æ–±–∫–∞—Ö [1], [2], [3] –∏ —Ç.–¥. –ï—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –≥–æ–≤–æ—Ä—è—Ç "
        "–æ–± –æ–¥–Ω–æ–º –∏ —Ç–æ–º –∂–µ, —Ç–æ —É–∫–∞–∑—ã–≤–∞–π—Ç–µ –≤—Å–µ. "
        "–î–ª—è —Å—Ç–∞—Ç–µ–π –∏ –Ω–æ–≤—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –∫—Ä–∞—Ç–∫–æ –æ–ø–∏—à–∏—Ç–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã: –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã. "
        "–í–ê–ñ–ù–û: –ù–ï –¥–æ–±–∞–≤–ª—è–π—Ç–µ –≤–≤–µ–¥–µ–Ω–∏–µ –∏ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ. –ù–∞—á–∏–Ω–∞–π—Ç–µ —Å—Ä–∞–∑—É —Å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –¥–∞–π–¥–∂–µ—Å—Ç–∞. "
        "–í–ê–ñ–ù–û: –£—á–∏—Ç—ã–≤–∞–π—Ç–µ, —á—Ç–æ Telegram-–∫–∞–Ω–∞–ª—ã –≤—ã—Ä–∞–∂–∞—é—Ç —Å–≤–æ–µ –º–Ω–µ–Ω–∏–µ –∏ –º–æ–≥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—É–±—ä–µ–∫—Ç–∏–≤–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏. "
        "–ï—Å–ª–∏ –∞–≤—Ç–æ—Ä –∫–∞–Ω–∞–ª–∞ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç —á—Ç–æ-—Ç–æ –∏–ª–∏ –¥–∞–µ—Ç —Å–æ–≤–µ—Ç—ã, –ù–ï –¥–æ—Å–ª–æ–≤–Ω–æ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–π—Ç–µ —ç—Ç–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏. "
        "–í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ –æ–ø–∏—à–∏—Ç–µ —Ñ–∞–∫—Ç—ã –∏ —Å–æ–±—ã—Ç–∏—è, –æ –∫–æ—Ç–æ—Ä—ã—Ö –∏–¥–µ—Ç —Ä–µ—á—å. "
        "–í–ê–ñ–ù–û: –ü–†–ò–î–ï–†–ñ–ò–í–ê–ô–¢–ï–°–¨ –ù–ï–ô–¢–†–ê–õ–¨–ù–û–ì–û –°–¢–ò–õ–Ø, –¥–∞–∂–µ –µ—Å–ª–∏ –∏—Å—Ö–æ–¥–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ, "
        "–≤–æ—Å—Ç–æ—Ä–∂–µ–Ω–Ω—ã–µ –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç –ø—Ä–µ—É–≤–µ–ª–∏—á–µ–Ω–∏—è. –ü–µ—Ä–µ–¥–∞–≤–∞–π—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ, –±–µ–∑ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –æ–∫—Ä–∞—Å–∫–∏. "
        "–í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å—Ç–∏–ª—å –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º —Å–æ–æ–±—â–µ–Ω–∏—è–º –∏–∑ Telegram-–∫–∞–Ω–∞–ª–æ–≤. "
        "–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Å—Ç–∏–ª—å –∏—Å—Ö–æ–¥–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏ —Å–æ–∑–¥–∞–≤–∞–π—Ç–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≤ —Ç–æ–º –∂–µ –¥—É—Ö–µ - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —ç–º–æ–¥–∑–∏, "
        "–∫–æ—Ä–æ—Ç–∫–∏–µ –∏ —è—Ä–∫–∏–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ –¥–ª—è Telegram. –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–µ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏. "
        "–í–ê–ñ–ù–û: –í –Ω–∞—á–∞–ª–µ –∫–∞–∂–¥–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –¥–æ–±–∞–≤–ª—è–π—Ç–µ 1-5 —ç–º–æ–¥–∑–∏, –∫–æ—Ç–æ—Ä—ã–µ –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ –∏ –º–µ—Ç–∞—Ñ–æ—Ä–∏—á–Ω–æ "
        "–æ–ø–∏—Å—ã–≤–∞—é—Ç —Å—É—Ç—å –Ω–æ–≤–æ—Å—Ç–∏. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–µ, –Ω–æ –ø–æ–Ω—è—Ç–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —ç–º–æ–¥–∑–∏. "
        "–≠–º–æ–¥–∑–∏ –ù–ï –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–∏–Ω–æ–Ω–∏–º–∏—á–Ω—ã–º–∏. "
        "–ü—Ä–∏–º–µ—Ä—ã —Å—Ç–∏–ª—è: '<b>üß†üíß –ü–µ—Ä–µ—Ö–æ–¥—ã –º–µ–∂–¥—É AI-–∫–æ–º–ø–∞–Ω–∏—è–º–∏</b>', "
        "'<b>ü§ñüé≠üëÖ –ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π</b>', "
        "'<b>ü´±üëÅÔ∏èü´≤ –ü—Ä–æ—Ä—ã–≤ –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏</b>', "
        "'<b>üí°üëÖ –ò–Ω–Ω–æ–≤–∞—Ü–∏–∏ –≤ NLP</b>', "
        "'<b>üéØü™øüìà –ù–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ Kaggle</b>', "
        "'<b>üíºüå™Ô∏èüìâ –†—ã–Ω–æ–∫ AI-–≤–∞–∫–∞–Ω—Å–∏–π</b>'. "
        "–í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¢–û–õ–¨–ö–û HTML-—Ç–µ–≥–∏ <b>—Ç–µ–∫—Å—Ç</b> –¥–ª—è –∂–∏—Ä–Ω–æ–≥–æ —à—Ä–∏—Ñ—Ç–∞, "
        "–ù–ï –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ **—Ç–µ–∫—Å—Ç** –∏–ª–∏ –¥—Ä—É–≥–∏–µ Markdown —Ä–∞–∑–º–µ—Ç–∫–∏. "
        "–í–ê–ñ–ù–û: –ù–ï —Å–æ–∑–¥–∞–≤–∞–π—Ç–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –≤ —Ç–µ–∫—Å—Ç–µ, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ–ª—å–∫–æ –Ω–æ–º–µ—Ä–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ [1], [2], [3] –∏ —Ç.–¥. "
        "–°—Å—ã–ª–∫–∏ –±—É–¥—É—Ç –¥–æ–±–∞–≤–ª–µ–Ω—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏. "
        f"–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–∞–º–º–∞—Ä–∏ –ù–ï –î–û–õ–ñ–ù–ê –ø—Ä–µ–≤—ã—à–∞—Ç—å {max_summary_length} —Å–∏–º–≤–æ–ª–æ–≤. "
        "–î–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π —Å–æ–∑–¥–∞–≤–∞–π—Ç–µ —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–µ —Å–∞–º–º–∞—Ä–∏, "
        "–Ω–æ —Å—Ç—Ä–æ–≥–æ —Å–æ–±–ª—é–¥–∞–π—Ç–µ –ª–∏–º–∏—Ç —Å–∏–º–≤–æ–ª–æ–≤."
        "denissexy - —ç—Ç–æ –∫–∞–Ω–∞–ª –ø—Ä–æ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç"
    )
    
    # –í—ã—á–∏—Å–ª—è–µ–º max_tokens –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã —Å–∞–º–º–∞—Ä–∏ (–ø—Ä–∏–º–µ—Ä–Ω–æ 4 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω)
    max_tokens = 16000
    
    result = await call_openai(system_prompt, messages_text, max_tokens=max_tokens)
    if not result:
        return "–û—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ–±–æ–±—â–µ–Ω–∏–µ"
    
    print(f"–î–ª–∏–Ω–∞ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: {total_original_length} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"–î–ª–∏–Ω–∞ —Å–∞–º–º–∞—Ä–∏: {count_characters(result)} —Å–∏–º–≤–æ–ª–æ–≤")
    
    # –ó–∞–º–µ–Ω—è–µ–º –Ω–æ–º–µ—Ä–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–∞ HTML-—Å—Å—ã–ª–∫–∏
    def replace_source_with_links(match):
        content = match.group(1)  # —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤–Ω—É—Ç—Ä–∏ —Å–∫–æ–±–æ–∫
        numbers = [num.strip() for num in content.split(',')]
        source_links = []
        
        for num_str in numbers:
            try:
                num = int(num_str)
                if 1 <= num <= len(messages):
                    msg = messages[num-1]
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å–æ–æ–±—â–µ–Ω–∏—è
                    links = extract_links(msg.text)
                    telegram_link = msg.get_telegram_link()
                    
                    # –í—Å–µ–≥–¥–∞ —Å–æ–∑–¥–∞–µ–º –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—É –∫–∞–Ω–∞–ª–∞
                    channel_abbr = create_channel_abbreviation(msg.channel)
                    
                    if links:
                        # –ï—Å–ª–∏ –µ—Å—Ç—å –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏, –¥–æ–±–∞–≤–ª—è–µ–º –∏ –≤–Ω–µ—à–Ω—é—é —Å—Å—ã–ª–∫—É, –∏ —Å—Å—ã–ª–∫—É –Ω–∞ Telegram-–ø–æ—Å—Ç
                        main_link = links[0]
                        source_links.append(
                            f'<a href="{main_link}">[{num}]</a> <a href="{telegram_link}">[{channel_abbr}]</a>'
                        )
                    else:
                        # –ï—Å–ª–∏ –≤–Ω–µ—à–Ω–∏—Ö —Å—Å—ã–ª–æ–∫ –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Å—Å—ã–ª–∫—É –Ω–∞ Telegram-—Å–æ–æ–±—â–µ–Ω–∏–µ —Å –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–æ–π
                        source_links.append(f'<a href="{telegram_link}">[{channel_abbr}]</a>')
                        
            except ValueError:
                continue
        return ', '.join(source_links)

    # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ [1], [1,2], [1,2,3] –∏ —Ç.–¥.
    all_sources_pattern = r'\[(\d+(?:,\s*\d+)*)\]'
    result = re.sub(all_sources_pattern, replace_source_with_links, result)

    return result


async def summarize_group_text(messages: List[MessageInfo]) -> str:
    """Call LLM to summarize group messages with community review header."""
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –Ω–æ–º–µ—Ä–æ–≤ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    messages_with_sources = []
    total_original_length = 0
    
    for i, msg in enumerate(messages, 1):
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å–æ–æ–±—â–µ–Ω–∏—è
        links = extract_links(msg.text)
        source_info = f"[{i}] {msg.text}"
        if links:
            source_info += f" (–°—Å—ã–ª–∫–∏: {', '.join(links)})"
        messages_with_sources.append(source_info)
        total_original_length += count_characters(msg.text)
    
    messages_text = "\n\n".join(messages_with_sources)
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É —Å–∞–º–º–∞—Ä–∏
    max_summary_length = min(total_original_length * 2, 16000)
    
    system_prompt = (
        "–û–±–æ–±—â–∏—Ç–µ —Å–ª–µ–¥—É—é—â–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ Telegram-–≥—Ä—É–ø–ø—ã –≤ –∫—Ä–∞—Ç–∫–∏–π –¥–∞–π–¥–∂–µ—Å—Ç, —Å—Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ NLP. "
        "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π—Ç–µ –¥–∞–π–¥–∂–µ—Å—Ç –ø–æ —Ç–µ–º–∞–º –∏–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –±–ª–æ–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —É–∫–∞–∑—ã–≤–∞–π—Ç–µ "
        "–Ω–æ–º–µ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –≤ –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã—Ö —Å–∫–æ–±–∫–∞—Ö [1], [2], [3] –∏ —Ç.–¥. –ï—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –≥–æ–≤–æ—Ä—è—Ç "
        "–æ–± –æ–¥–Ω–æ–º –∏ —Ç–æ–º –∂–µ, —Ç–æ —É–∫–∞–∑—ã–≤–∞–π—Ç–µ –≤—Å–µ. "
        "–î–ª—è —Å—Ç–∞—Ç–µ–π –∏ –Ω–æ–≤—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –∫—Ä–∞—Ç–∫–æ –æ–ø–∏—à–∏—Ç–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã: –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã. "
        "–í–ê–ñ–ù–û: –ù–ï –¥–æ–±–∞–≤–ª—è–π—Ç–µ –≤–≤–µ–¥–µ–Ω–∏–µ –∏ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ. –ù–∞—á–∏–Ω–∞–π—Ç–µ —Å—Ä–∞–∑—É —Å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –¥–∞–π–¥–∂–µ—Å—Ç–∞. "
        "–í–ê–ñ–ù–û: –£—á–∏—Ç—ã–≤–∞–π—Ç–µ, —á—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ –≥—Ä—É–ø–ø –º–æ–≥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –æ–±—Å—É–∂–¥–µ–Ω–∏—è, –≤–æ–ø—Ä–æ—Å—ã –∏ –º–Ω–µ–Ω–∏—è —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤. "
        "–ï—Å–ª–∏ —É—á–∞—Å—Ç–Ω–∏–∫–∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é—Ç —á—Ç–æ-—Ç–æ –∏–ª–∏ –¥–∞—é—Ç —Å–æ–≤–µ—Ç—ã, –ù–ï –¥–æ—Å–ª–æ–≤–Ω–æ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–π—Ç–µ —ç—Ç–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏. "
        "–í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ –æ–ø–∏—à–∏—Ç–µ —Ñ–∞–∫—Ç—ã –∏ —Å–æ–±—ã—Ç–∏—è, –æ –∫–æ—Ç–æ—Ä—ã—Ö –∏–¥–µ—Ç —Ä–µ—á—å. "
        "–í–ê–ñ–ù–û: –ü–†–ò–î–ï–†–ñ–ò–í–ê–ô–¢–ï–°–¨ –ù–ï–ô–¢–†–ê–õ–¨–ù–û–ì–û –°–¢–ò–õ–Ø, –¥–∞–∂–µ –µ—Å–ª–∏ –∏—Å—Ö–æ–¥–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ, "
        "–≤–æ—Å—Ç–æ—Ä–∂–µ–Ω–Ω—ã–µ –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç –ø—Ä–µ—É–≤–µ–ª–∏—á–µ–Ω–∏—è. –ü–µ—Ä–µ–¥–∞–≤–∞–π—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ, –±–µ–∑ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –æ–∫—Ä–∞—Å–∫–∏. "
        "–í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å—Ç–∏–ª—å –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º —Å–æ–æ–±—â–µ–Ω–∏—è–º –∏–∑ Telegram-–≥—Ä—É–ø–ø. "
        "–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Å—Ç–∏–ª—å –∏—Å—Ö–æ–¥–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏ —Å–æ–∑–¥–∞–≤–∞–π—Ç–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≤ —Ç–æ–º –∂–µ –¥—É—Ö–µ - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —ç–º–æ–¥–∑–∏, "
        "–∫–æ—Ä–æ—Ç–∫–∏–µ –∏ —è—Ä–∫–∏–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ –¥–ª—è Telegram. –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–µ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏. "
        "–í–ê–ñ–ù–û: –í –Ω–∞—á–∞–ª–µ –∫–∞–∂–¥–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –¥–æ–±–∞–≤–ª—è–π—Ç–µ 1-5 —ç–º–æ–¥–∑–∏, –∫–æ—Ç–æ—Ä—ã–µ –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ –∏ –º–µ—Ç–∞—Ñ–æ—Ä–∏—á–Ω–æ "
        "–æ–ø–∏—Å—ã–≤–∞—é—Ç —Å—É—Ç—å –Ω–æ–≤–æ—Å—Ç–∏. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–µ, –Ω–æ –ø–æ–Ω—è—Ç–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —ç–º–æ–¥–∑–∏. "
        "–≠–º–æ–¥–∑–∏ –ù–ï –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–∏–Ω–æ–Ω–∏–º–∏—á–Ω—ã–º–∏. "
        "–ü—Ä–∏–º–µ—Ä—ã —Å—Ç–∏–ª—è: '<b>üß†üíß –ü–µ—Ä–µ—Ö–æ–¥—ã –º–µ–∂–¥—É AI-–∫–æ–º–ø–∞–Ω–∏—è–º–∏</b>', "
        "'<b>ü§ñüé≠üëÖ –ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π</b>', "
        "'<b>ü´±üëÅÔ∏èü´≤ –ü—Ä–æ—Ä—ã–≤ –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏</b>', "
        "'<b>üí°üëÖ –ò–Ω–Ω–æ–≤–∞—Ü–∏–∏ –≤ NLP</b>', "
        "'<b>üéØü™øüìà –ù–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ Kaggle</b>', "
        "'<b>üíºüå™Ô∏èüìâ –†—ã–Ω–æ–∫ AI-–≤–∞–∫–∞–Ω—Å–∏–π</b>'. "
        "–í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¢–û–õ–¨–ö–û HTML-—Ç–µ–≥–∏ <b>—Ç–µ–∫—Å—Ç</b> –¥–ª—è –∂–∏—Ä–Ω–æ–≥–æ —à—Ä–∏—Ñ—Ç–∞, "
        "–ù–ï –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ **—Ç–µ–∫—Å—Ç** –∏–ª–∏ –¥—Ä—É–≥–∏–µ Markdown —Ä–∞–∑–º–µ—Ç–∫–∏. "
        "–í–ê–ñ–ù–û: –ù–ï —Å–æ–∑–¥–∞–≤–∞–π—Ç–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –≤ —Ç–µ–∫—Å—Ç–µ, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ–ª—å–∫–æ –Ω–æ–º–µ—Ä–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ [1], [2], [3] –∏ —Ç.–¥. "
        "–°—Å—ã–ª–∫–∏ –±—É–¥—É—Ç –¥–æ–±–∞–≤–ª–µ–Ω—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏. "
        f"–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–∞–º–º–∞—Ä–∏ –ù–ï –î–û–õ–ñ–ù–ê –ø—Ä–µ–≤—ã—à–∞—Ç—å {max_summary_length} —Å–∏–º–≤–æ–ª–æ–≤. "
        "–î–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π —Å–æ–∑–¥–∞–≤–∞–π—Ç–µ —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–µ —Å–∞–º–º–∞—Ä–∏, "
        "–Ω–æ —Å—Ç—Ä–æ–≥–æ —Å–æ–±–ª—é–¥–∞–π—Ç–µ –ª–∏–º–∏—Ç —Å–∏–º–≤–æ–ª–æ–≤."
    )
    
    # –í—ã—á–∏—Å–ª—è–µ–º max_tokens –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã —Å–∞–º–º–∞—Ä–∏ (–ø—Ä–∏–º–µ—Ä–Ω–æ 4 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω)
    max_tokens = 16000
    
    result = await call_openai(system_prompt, messages_text, max_tokens=max_tokens)
    if not result:
        return "–û—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ–±–æ–±—â–µ–Ω–∏–µ"
    
    print(f"–î–ª–∏–Ω–∞ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≥—Ä—É–ø–ø: {total_original_length} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"–î–ª–∏–Ω–∞ —Å–∞–º–º–∞—Ä–∏ –≥—Ä—É–ø–ø: {count_characters(result)} —Å–∏–º–≤–æ–ª–æ–≤")
    
    # –ó–∞–º–µ–Ω—è–µ–º –Ω–æ–º–µ—Ä–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–∞ HTML-—Å—Å—ã–ª–∫–∏
    def replace_source_with_links(match):
        content = match.group(1)  # —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤–Ω—É—Ç—Ä–∏ —Å–∫–æ–±–æ–∫
        numbers = [num.strip() for num in content.split(',')]
        source_links = []
        
        for num_str in numbers:
            try:
                num = int(num_str)
                if 1 <= num <= len(messages):
                    msg = messages[num-1]
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å–æ–æ–±—â–µ–Ω–∏—è
                    links = extract_links(msg.text)
                    telegram_link = msg.get_telegram_link()
                    
                    # –í—Å–µ–≥–¥–∞ —Å–æ–∑–¥–∞–µ–º –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—É –∫–∞–Ω–∞–ª–∞
                    channel_abbr = create_channel_abbreviation(msg.channel)
                    
                    if links:
                        # –ï—Å–ª–∏ –µ—Å—Ç—å –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏, –¥–æ–±–∞–≤–ª—è–µ–º –∏ –≤–Ω–µ—à–Ω—é—é —Å—Å—ã–ª–∫—É, –∏ —Å—Å—ã–ª–∫—É –Ω–∞ Telegram-–ø–æ—Å—Ç
                        main_link = links[0]
                        source_links.append(
                            f'<a href="{main_link}">[{num}]</a> <a href="{telegram_link}">[{channel_abbr}]</a>'
                        )
                    else:
                        # –ï—Å–ª–∏ –≤–Ω–µ—à–Ω–∏—Ö —Å—Å—ã–ª–æ–∫ –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Å—Å—ã–ª–∫—É –Ω–∞ Telegram-—Å–æ–æ–±—â–µ–Ω–∏–µ —Å –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–æ–π
                        source_links.append(f'<a href="{telegram_link}">[{channel_abbr}]</a>')
                        
            except ValueError:
                continue
        return ', '.join(source_links)

    # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ [1], [1,2], [1,2,3] –∏ —Ç.–¥.
    all_sources_pattern = r'\[(\d+(?:,\s*\d+)*)\]'
    result = re.sub(all_sources_pattern, replace_source_with_links, result)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ "–û–±–∑–æ—Ä —Å–æ–æ–±—â–µ—Å—Ç–≤–∞"
    group_names = list(set(msg.channel.lstrip('@') for msg in messages))
    community_name = ', '.join(group_names)
    header = f"<b>üë• –û–±–∑–æ—Ä —Å–æ–æ–±—â–µ—Å—Ç–≤–∞ {community_name}</b>\n\n"
    
    return header + result


async def fetch_messages():
    """Fetch messages from source channels in the last 24 hours."""
    since = datetime.now(timezone.utc) - timedelta(days=1)
    all_msgs = []
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
    processed_messages = load_summarization_history()
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(processed_messages)} —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏")
    
    # –ü–æ–ª—É—á–∞–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∫–∞–Ω–∞–ª–æ–≤
    all_channels = get_all_source_channels()
    
    for channel in all_channels:
        print(f"Fetching messages from {channel}...")
        channel_msgs = []
        async for msg in user_client.iter_messages(channel, offset_date=None, min_id=0, reverse=False):
            if msg.date < since:
                break
            if msg.message:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å–æ–æ–±—â–µ–Ω–∏—è
                links = extract_links(msg.message)
                main_link = links[0] if links else ""
                
                message_info = MessageInfo(
                    text=msg.message,
                    channel=channel,
                    message_id=msg.id,
                    date=msg.date,
                    link=main_link
                )
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –±—ã–ª–æ –ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ
                if not is_message_processed(message_info, processed_messages):
                    channel_msgs.append(message_info)
                    all_msgs.append(message_info)
                else:
                    print(f"  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ {msg.id} –∏–∑ {channel}")
                    
        print(f"  Found {len(channel_msgs)} –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π from {channel}")
    return all_msgs


async def fetch_group_messages():
    """Fetch messages from source groups in the last 24 hours."""
    since = datetime.now(timezone.utc) - timedelta(days=1)
    all_msgs = []
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ –≥—Ä—É–ø–ø
    processed_messages = load_group_summarization_history()
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(processed_messages)} —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ –≥—Ä—É–ø–ø –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏")
    
    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≥—Ä—É–ø–ø
    all_groups = SOURCE_GROUPS
    
    for group in all_groups:
        print(f"Fetching messages from group {group}...")
        group_msgs = []
        async for msg in user_client.iter_messages(group, offset_date=None, min_id=0, reverse=False):
            if msg.date < since:
                break
            if msg.message:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å–æ–æ–±—â–µ–Ω–∏—è
                links = extract_links(msg.message)
                main_link = links[0] if links else ""
                
                message_info = MessageInfo(
                    text=msg.message,
                    channel=group,
                    message_id=msg.id,
                    date=msg.date,
                    link=main_link
                )
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –±—ã–ª–æ –ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ
                if not is_message_processed(message_info, processed_messages):
                    group_msgs.append(message_info)
                    all_msgs.append(message_info)
                else:
                    print(f"  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ {msg.id} –∏–∑ –≥—Ä—É–ø–ø—ã {group}")
                    
        print(f"  Found {len(group_msgs)} –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π from group {group}")
    return all_msgs


async def remove_duplicates(messages: List[MessageInfo]) -> List[MessageInfo]:
    unique_msgs: List[MessageInfo] = []
    seen_links = set()
    
    for msg in messages:
        links = extract_links(msg.text)
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ —Å—Å—ã–ª–∫–∞–º - –µ—Å–ª–∏ —Å—Å—ã–ª–∫–∞ —É–∂–µ –±—ã–ª–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        if links and any(link in seen_links for link in links):
            print(f"  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç –ø–æ —Å—Å—ã–ª–∫–µ: {links[0]}")
            continue
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ —Ç–µ–∫—Å—Ç—É
        duplicate = False
        for u in unique_msgs:
            if SequenceMatcher(None, msg.text, u.text).ratio() > SIMILARITY_THRESHOLD:
                print(f"  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç –ø–æ —Ç–µ–∫—Å—Ç—É: {msg.text[:50]}...")
                duplicate = True
                break
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –¥—É–±–ª–∏–∫–∞—Ç –ø–æ —Ç–µ–∫—Å—Ç—É, –ø—Ä–æ–≤–µ—Ä—è–µ–º —á–µ—Ä–µ–∑ LLM
        if not duplicate:
            for u in unique_msgs:
                try:
                    if await are_messages_duplicate(msg, u):
                        print(f"  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç –ø–æ LLM: {msg.text[:50]}...")
                        duplicate = True
                        break
                except Exception as e:
                    print(f"  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –¥—É–±–ª–∏–∫–∞—Ç–∞ —á–µ—Ä–µ–∑ LLM: {e}")
                    # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ LLM, —Å—á–∏—Ç–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è —Ä–∞–∑–Ω—ã–º–∏
                    continue
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –±—ã–ª–∞ –ª–∏ —Ç–µ–º–∞ —É–∂–µ –æ—Å–≤–µ—â–µ–Ω–∞ –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–∞–º–º–∞—Ä–∏
        if not duplicate and ENABLE_SUMMARIES_DEDUPLICATION:
            try:
                if await is_message_covered_in_summaries(msg):
                    print(f"  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ, —É–∂–µ –æ—Å–≤–µ—â–µ–Ω–Ω–æ–µ –≤ —Å–∞–º–º–∞—Ä–∏: {msg.text[:50]}...")
                    duplicate = True
            except Exception as e:
                print(f"  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –ø–æ–∫—Ä—ã—Ç–∏—è –≤ —Å–∞–º–º–∞—Ä–∏: {e}")
                # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏, —Å—á–∏—Ç–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –Ω–æ–≤—ã–º
                pass
        
        if not duplicate:
            unique_msgs.append(msg)
            seen_links.update(links)
            print(f"  –î–æ–±–∞–≤–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ: {msg.text[:50]}...")
    
    return unique_msgs


async def remove_group_duplicates(messages: List[MessageInfo]) -> List[MessageInfo]:
    """Remove duplicates from group messages, checking against group summaries history."""
    unique_msgs: List[MessageInfo] = []
    seen_links = set()
    
    for msg in messages:
        links = extract_links(msg.text)
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ —Å—Å—ã–ª–∫–∞–º - –µ—Å–ª–∏ —Å—Å—ã–ª–∫–∞ —É–∂–µ –±—ã–ª–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        if links and any(link in seen_links for link in links):
            print(f"  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç –ø–æ —Å—Å—ã–ª–∫–µ: {links[0]}")
            continue
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ —Ç–µ–∫—Å—Ç—É
        duplicate = False
        for u in unique_msgs:
            if SequenceMatcher(None, msg.text, u.text).ratio() > SIMILARITY_THRESHOLD:
                print(f"  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç –ø–æ —Ç–µ–∫—Å—Ç—É: {msg.text[:50]}...")
                duplicate = True
                break
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –¥—É–±–ª–∏–∫–∞—Ç –ø–æ —Ç–µ–∫—Å—Ç—É, –ø—Ä–æ–≤–µ—Ä—è–µ–º —á–µ—Ä–µ–∑ LLM
        if not duplicate:
            for u in unique_msgs:
                try:
                    if await are_messages_duplicate(msg, u):
                        print(f"  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç –ø–æ LLM: {msg.text[:50]}...")
                        duplicate = True
                        break
                except Exception as e:
                    print(f"  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –¥—É–±–ª–∏–∫–∞—Ç–∞ —á–µ—Ä–µ–∑ LLM: {e}")
                    # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ LLM, —Å—á–∏—Ç–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è —Ä–∞–∑–Ω—ã–º–∏
                    continue
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –±—ã–ª–∞ –ª–∏ —Ç–µ–º–∞ —É–∂–µ –æ—Å–≤–µ—â–µ–Ω–∞ –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–∞–º–º–∞—Ä–∏ –≥—Ä—É–ø–ø
        if not duplicate and ENABLE_SUMMARIES_DEDUPLICATION:
            try:
                if await is_message_covered_in_group_summaries(msg):
                    print(f"  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ, —É–∂–µ –æ—Å–≤–µ—â–µ–Ω–Ω–æ–µ –≤ —Å–∞–º–º–∞—Ä–∏ –≥—Ä—É–ø–ø: {msg.text[:50]}...")
                    duplicate = True
            except Exception as e:
                print(f"  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –ø–æ–∫—Ä—ã—Ç–∏—è –≤ —Å–∞–º–º–∞—Ä–∏ –≥—Ä—É–ø–ø: {e}")
                # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏, —Å—á–∏—Ç–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –Ω–æ–≤—ã–º
                pass
        
        if not duplicate:
            unique_msgs.append(msg)
            seen_links.update(links)
            print(f"  –î–æ–±–∞–≤–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –∏–∑ –≥—Ä—É–ø–ø—ã: {msg.text[:50]}...")
    
    return unique_msgs


async def main():
    # Start both clients
    await user_client.start()
    await bot_client.start(bot_token=BOT_TOKEN)
    
    try:
        # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø–æ—Ö–æ–∂–∏—Ö –∫–∞–Ω–∞–ª–æ–≤
        # –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–æ–∫—É –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è
        # await discover_and_save_similar_channels()
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–Ω–∞–ª–æ–≤ (–∫–∞–∫ –æ–±—ã—á–Ω–æ)
        print("=== –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–Ω–∞–ª–æ–≤ ===")
        messages = await fetch_messages()
        print(f"Fetched {len(messages)} new messages from channels")
        
        if messages:
            # Apply NLP filtering to remove advertising
            filtered = []
            all_checked_messages = []  # –í—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã —á–µ—Ä–µ–∑ is_nlp_related
            discovered_channels = set()  # –ö–∞–Ω–∞–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ —Ä–µ–ø–æ—Å—Ç–Ω—É–ª–∏ –∏ –ø—Ä–æ—à–ª–∏ –ø—Ä–æ–≤–µ—Ä–∫—É
            
            for i, msg in enumerate(messages):
                print(f"Checking message {i+1}/{len(messages)}...")
                # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                all_checked_messages.append(msg)
                
                if await is_nlp_related(msg.text):
                    filtered.append(msg)
                    # –ï—Å–ª–∏ –∫–∞–Ω–∞–ª –Ω–µ –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞ SOURCE_CHANNELS, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ –≤ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ
                    if msg.channel not in SOURCE_CHANNELS:
                        discovered_channels.add(msg.channel)
                    print(f"  ‚úì Message {i+1} is NLP-related: {msg.text[:100]}; {msg.link}")
                else:
                    print(f"  ‚úó Message {i+1} is not NLP-related (likely advertising): {msg.text[:100]}; "
                          f"{msg.channel}; {msg.link}")
            print(f"{len(filtered)} messages after NLP filter")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –∏—Å—Ç–æ—Ä–∏—é
            save_summarization_history(all_checked_messages)
            print(f"Saved {len(all_checked_messages)} checked messages to history")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—ã–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –∫–∞–Ω–∞–ª—ã
            for channel in discovered_channels:
                save_discovered_channel(channel)
            if discovered_channels:
                print(f"Discovered {len(discovered_channels)} new channels: {', '.join(discovered_channels)}")
            
            if filtered:
                unique = await remove_duplicates(filtered)
                print(f"{len(unique)} messages after deduplication")
                
                if unique:
                    summary = await summarize_text(unique)
                    await user_client.send_message(TARGET_CHANNEL, summary, parse_mode='html')
                    print("Channel summary sent")
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∞–º–º–∞—Ä–∏ –≤ –∏—Å—Ç–æ—Ä–∏—é
                    channels = list(set(msg.channel for msg in unique))
                    summary_info = SummaryInfo(
                        content=summary,
                        date=datetime.now(timezone.utc),
                        message_count=len(unique),
                        channels=channels
                    )
                    save_summary_to_history(summary_info)
                    print(f"Channel summary saved to history (channels: {channels})")
                else:
                    print("No unique NLP messages found in channels")
            else:
                print("No new NLP-related messages found in channels")
        else:
            print("No new messages found in channels")
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥—Ä—É–ø–ø (—Ä–∞–∑ –≤ —Å—É—Ç–∫–∏)
        print("\n=== –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥—Ä—É–ø–ø ===")
        if SOURCE_GROUPS and should_run_group_summarization():
            print("Starting daily group summarization...")
            
            group_messages = await fetch_group_messages()
            print(f"Fetched {len(group_messages)} new messages from groups")
            
            if group_messages:
                # Apply NLP filtering to remove advertising
                group_filtered = []
                all_checked_group_messages = []
                
                for i, msg in enumerate(group_messages):
                    print(f"Checking group message {i+1}/{len(group_messages)}...")
                    all_checked_group_messages.append(msg)
                    
                    if await is_nlp_related(msg.text):
                        group_filtered.append(msg)
                        print(f"  ‚úì Group message {i+1} is NLP-related: {msg.text[:100]}; {msg.link}")
                    else:
                        print(f"  ‚úó Group message {i+1} is not NLP-related: {msg.text[:100]}; {msg.link}")
                print(f"{len(group_filtered)} group messages after NLP filter")
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ –≥—Ä—É–ø–ø –≤ –∏—Å—Ç–æ—Ä–∏—é
                save_group_summarization_history(all_checked_group_messages)
                print(f"Saved {len(all_checked_group_messages)} checked group messages to history")
                
                if group_filtered:
                    unique_group = await remove_group_duplicates(group_filtered)
                    print(f"{len(unique_group)} group messages after deduplication")
                    
                    if unique_group:
                        group_summary = await summarize_group_text(unique_group)
                        await user_client.send_message(TARGET_CHANNEL, group_summary, parse_mode='html')
                        print("Group summary sent")
                        
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∞–º–º–∞—Ä–∏ –≥—Ä—É–ø–ø –≤ –∏—Å—Ç–æ—Ä–∏—é
                        groups = list(set(msg.channel for msg in unique_group))
                        group_summary_info = SummaryInfo(
                            content=group_summary,
                            date=datetime.now(timezone.utc),
                            message_count=len(unique_group),
                            channels=groups
                        )
                        save_group_summary_to_history(group_summary_info)
                        print(f"Group summary saved to history (groups: {groups})")
                        
                        # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–∞–ø—É—Å–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –≥—Ä—É–ø–ø
                        update_group_last_run()
                        print("Group summarization completed for today")
                    else:
                        print("No unique NLP messages found in groups")
                else:
                    print("No new NLP-related messages found in groups")
            else:
                print("No new messages found in groups")
        else:
            if SOURCE_GROUPS:
                print("Group summarization skipped (already run today)")
            else:
                print("No groups configured for summarization")
        
    finally:
        # Disconnect both clients
        await user_client.disconnect()
        await bot_client.disconnect()


if __name__ == "__main__":
    asyncio.run(main())
