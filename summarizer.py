import os
import asyncio
import json
from datetime import datetime, timedelta, timezone
from difflib import SequenceMatcher
import re
from dataclasses import dataclass
from typing import List, Set

from dotenv import load_dotenv
from telethon import TelegramClient
from openai import OpenAI

load_dotenv()

# Get environment variables with proper error handling
api_id_str = os.getenv('TELEGRAM_API_ID')
if not api_id_str:
    raise ValueError("TELEGRAM_API_ID environment variable is required")
API_ID = int(api_id_str)

api_hash = os.getenv('TELEGRAM_API_HASH')
if not api_hash:
    raise ValueError("TELEGRAM_API_HASH environment variable is required")
API_HASH = api_hash

bot_token = os.getenv('TELEGRAM_BOT_TOKEN')
if not bot_token:
    raise ValueError("TELEGRAM_BOT_TOKEN environment variable is required")
BOT_TOKEN = bot_token

target_channel = os.getenv('TARGET_CHANNEL')
if not target_channel:
    raise ValueError("TARGET_CHANNEL environment variable is required")
TARGET_CHANNEL = target_channel

openai_api_key = os.getenv('OPENAI_API_KEY')
if not openai_api_key:
    raise ValueError("OPENAI_API_KEY environment variable is required")
OPENAI_API_KEY = openai_api_key

source_channels_str = os.getenv('SOURCE_CHANNELS', '')
SOURCE_CHANNELS = [c.strip() for c in source_channels_str.split(',') if c.strip()]

openai_client = OpenAI(api_key=OPENAI_API_KEY)

LINK_REGEX = re.compile(r"https?://\S+")

# Create separate clients for user (reading) and bot (sending)
user_client = TelegramClient('tg_summarizer_user', API_ID, API_HASH)
bot_client = TelegramClient('tg_summarizer_bot', API_ID, API_HASH)

SIMILARITY_THRESHOLD = 0.9
HISTORY_FILE = 'summarization_history.json'
SUMMARIES_HISTORY_FILE = 'summaries_history.json'
# –§–ª–∞–≥ –¥–ª—è –æ—Ç–∫–ª—é—á–µ–Ω–∏—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ–∫—Ä—ã—Ç–∏—è –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–∞–º–º–∞—Ä–∏
ENABLE_SUMMARIES_DEDUPLICATION = True


@dataclass
class MessageInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–æ–æ–±—â–µ–Ω–∏–∏ Telegram"""
    text: str
    channel: str
    message_id: int
    date: datetime
    link: str
    
    def get_telegram_link(self) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—Å—ã–ª–∫—É –Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ Telegram"""
        # –£–±–∏—Ä–∞–µ–º @ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–∞–Ω–∞–ª–∞ –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è —Å—Å—ã–ª–∫–∏
        channel_name = self.channel.lstrip('@')
        return f"https://t.me/{channel_name}/{self.message_id}"
    
    def to_dict(self) -> dict:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –æ–±—ä–µ–∫—Ç –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ JSON"""
        return {
            'text': self.text,
            'channel': self.channel,
            'message_id': self.message_id,
            'date': self.date.isoformat(),
            'link': self.link
        }
    
    @classmethod
    def from_dict(cls, data: dict) -> 'MessageInfo':
        """–°–æ–∑–¥–∞–µ—Ç –æ–±—ä–µ–∫—Ç –∏–∑ —Å–ª–æ–≤–∞—Ä—è"""
        return cls(
            text=data['text'],
            channel=data['channel'],
            message_id=data['message_id'],
            date=datetime.fromisoformat(data['date']),
            link=data['link']
        )


@dataclass
class SummaryInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–æ–∑–¥–∞–Ω–Ω–æ–º —Å–∞–º–º–∞—Ä–∏"""
    content: str
    date: datetime
    message_count: int
    channels: List[str]
    
    def to_dict(self) -> dict:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –æ–±—ä–µ–∫—Ç –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ JSON"""
        return {
            'content': self.content,
            'date': self.date.isoformat(),
            'message_count': self.message_count,
            'channels': self.channels
        }
    
    @classmethod
    def from_dict(cls, data: dict) -> 'SummaryInfo':
        """–°–æ–∑–¥–∞–µ—Ç –æ–±—ä–µ–∫—Ç –∏–∑ —Å–ª–æ–≤–∞—Ä—è"""
        return cls(
            content=data['content'],
            date=datetime.fromisoformat(data['date']),
            message_count=data['message_count'],
            channels=data['channels']
        )


def count_characters(text: str) -> int:
    """–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ, –∏—Å–∫–ª—é—á–∞—è HTML-—Ç–µ–≥–∏."""
    # –£–¥–∞–ª—è–µ–º HTML-—Ç–µ–≥–∏ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –ø–æ–¥—Å—á–µ—Ç–∞ —Å–∏–º–≤–æ–ª–æ–≤
    import re
    clean_text = re.sub(r'<[^>]+>', '', text)
    return len(clean_text)


async def call_openai(system_prompt: str, user_content: str, max_tokens: int = 300) -> str:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–∑–æ–≤–∞ OpenAI API."""
    try:
        response = openai_client.chat.completions.create(
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ],
            model="gpt-4o-mini",
            max_tokens=max_tokens,
        )
        result = response.choices[0].message.content
        if result is None:
            return ""
        return result.strip()
    except Exception as e:
        print(f"OpenAI API error: {e}")
        return ""


def extract_links(text: str) -> list[str]:
    """Return all URLs from a string."""
    return LINK_REGEX.findall(text)


def load_channel_abbreviations() -> dict:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –∫–∞–Ω–∞–ª–æ–≤ –∏–∑ JSON —Ñ–∞–π–ª–∞."""
    abbreviations_file = 'channel_abbreviations.json'
    if not os.path.exists(abbreviations_file):
        return {}
    
    try:
        with open(abbreviations_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get('channel_abbreviations', {})
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä –∫–∞–Ω–∞–ª–æ–≤: {e}")
        return {}


def save_channel_abbreviation(channel_name: str, abbreviation: str) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–æ–≤—É—é –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—É –∫–∞–Ω–∞–ª–∞ –≤ JSON —Ñ–∞–π–ª."""
    abbreviations_file = 'channel_abbreviations.json'
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
        if os.path.exists(abbreviations_file):
            with open(abbreviations_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
        else:
            data = {'channel_abbreviations': {}, 'last_updated': ''}
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—É
        data['channel_abbreviations'][channel_name] = abbreviation
        data['last_updated'] = datetime.now().isoformat()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        with open(abbreviations_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
            
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –∫–∞–Ω–∞–ª–∞: {e}")


def create_channel_abbreviation(channel_name: str) -> str:
    """–°–æ–∑–¥–∞–µ—Ç –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—É –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–∞–Ω–∞–ª–∞."""
    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
    existing_abbreviations = load_channel_abbreviations()
    
    if channel_name in existing_abbreviations:
        return existing_abbreviations[channel_name]
    
    # –ï—Å–ª–∏ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –Ω–µ—Ç, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é
    clean_name = channel_name.lstrip('@')
    words = re.split(r'[\s\-_]+', clean_name)
    abbreviation = ''.join(word[0].upper() for word in words if word)
    
    # –ï—Å–ª–∏ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–∞—è, –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3-4 –±—É–∫–≤—ã
    if len(abbreviation) > 4:
        abbreviation = abbreviation[:4]
    
    # –ï—Å–ª–∏ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞ –ø—É—Å—Ç–∞—è –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–µ –±—É–∫–≤—ã –Ω–∞–∑–≤–∞–Ω–∏—è
    if len(abbreviation) < 2:
        abbreviation = clean_name[:3].upper()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç—É–µ—Ç –ª–∏ –Ω–æ–≤–∞—è –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏
    existing_values = set(existing_abbreviations.values())
    if abbreviation in existing_values:
        # –ï—Å–ª–∏ –∫–æ–Ω—Ñ–ª–∏–∫—Ç, –¥–æ–±–∞–≤–ª—è–µ–º —Ü–∏—Ñ—Ä—É
        counter = 1
        while f"{abbreviation}{counter}" in existing_values:
            counter += 1
        abbreviation = f"{abbreviation}{counter}"
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—É—é –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—É
    save_channel_abbreviation(channel_name, abbreviation)
    
    return abbreviation


def load_summarization_history() -> Set[str]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ —Ñ–∞–π–ª–∞."""
    if not os.path.exists(HISTORY_FILE):
        return set()
    
    try:
        with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
            processed_messages = set()
            for msg_data in data.get('processed_messages', []):
                msg = MessageInfo.from_dict(msg_data)
                # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä: –∫–∞–Ω–∞–ª + message_id + —Ö–µ—à —Ç–µ–∫—Å—Ç–∞
                msg_id = f"{msg.channel}_{msg.message_id}_{hash(msg.text)}"
                processed_messages.add(msg_id)
            return processed_messages
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏—Å—Ç–æ—Ä–∏–∏: {e}")
        return set()


def save_summarization_history(messages: List[MessageInfo]) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –∏—Å—Ç–æ—Ä–∏—é."""
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∏—Å—Ç–æ—Ä–∏—é
        existing_data = []
        if os.path.exists(HISTORY_FILE):
            with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                existing_data = data.get('processed_messages', [])
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        new_messages = [msg.to_dict() for msg in messages]
        all_messages = existing_data + new_messages
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏ 1000 —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏
        if len(all_messages) > 1000:
            all_messages = all_messages[-1000:]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é
        with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
            json.dump({
                'processed_messages': all_messages,
                'last_updated': datetime.now().isoformat()
            }, f, ensure_ascii=False, indent=2)
            
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∏—Å—Ç–æ—Ä–∏–∏: {e}")


def load_summaries_history() -> List[SummaryInfo]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —Å–∞–º–º–∞—Ä–∏ –∏–∑ —Ñ–∞–π–ª–∞."""
    if not os.path.exists(SUMMARIES_HISTORY_FILE):
        return []
    
    try:
        with open(SUMMARIES_HISTORY_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            summaries = []
            for summary_data in data.get('summaries', []):
                summaries.append(SummaryInfo.from_dict(summary_data))
            return summaries
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏—Å—Ç–æ—Ä–∏–∏ —Å–∞–º–º–∞—Ä–∏: {e}")
        return []


def save_summary_to_history(summary: SummaryInfo) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–æ–≤–æ–µ —Å–∞–º–º–∞—Ä–∏ –≤ –∏—Å—Ç–æ—Ä–∏—é."""
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∏—Å—Ç–æ—Ä–∏—é
        existing_summaries = []
        if os.path.exists(SUMMARIES_HISTORY_FILE):
            with open(SUMMARIES_HISTORY_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                existing_summaries = data.get('summaries', [])
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤–æ–µ —Å–∞–º–º–∞—Ä–∏
        new_summary = summary.to_dict()
        all_summaries = existing_summaries + [new_summary]
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏ 50 —Å–∞–º–º–∞—Ä–∏
        if len(all_summaries) > 50:
            all_summaries = all_summaries[-50:]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é
        with open(SUMMARIES_HISTORY_FILE, 'w', encoding='utf-8') as f:
            json.dump({
                'summaries': all_summaries,
                'last_updated': datetime.now().isoformat()
            }, f, ensure_ascii=False, indent=2)
            
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∏—Å—Ç–æ—Ä–∏–∏ —Å–∞–º–º–∞—Ä–∏: {e}")


def get_recent_summaries_context(days: int = 7) -> str:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–∞–º–º–∞—Ä–∏ –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏."""
    summaries = load_summaries_history()
    if not summaries:
        return ""
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–∞–º–º–∞—Ä–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –¥–Ω–µ–π
    cutoff_date = datetime.now(timezone.utc) - timedelta(days=days)
    recent_summaries = [s for s in summaries if s.date >= cutoff_date]
    
    if not recent_summaries:
        return ""
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 3 —Å–∞–º–º–∞—Ä–∏
    context_parts = []
    for i, summary in enumerate(recent_summaries[-3:], 1):
        # –û—á–∏—â–∞–µ–º HTML —Ç–µ–≥–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        clean_content = re.sub(r'<[^>]+>', '', summary.content)
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        context_parts.append(f"–°–∞–º–º–∞—Ä–∏ {i} ({summary.date.strftime('%Y-%m-%d')}):\n{clean_content[:500]}...")
    
    return "\n\n".join(context_parts)


def is_message_processed(msg: MessageInfo, processed_messages: Set[str]) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –±—ã–ª–æ –ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ä–∞–Ω–µ–µ."""
    msg_id = f"{msg.channel}_{msg.message_id}_{hash(msg.text)}"
    return msg_id in processed_messages


async def are_messages_duplicate(msg_a: MessageInfo, msg_b: MessageInfo) -> bool:
    """Use the LLM to see if two messages cover the same topic."""
    system_prompt = (
        "–û–ø–∏—Å—ã–≤–∞—é—Ç –ª–∏ —Å–ª–µ–¥—É—é—â–∏–µ –¥–≤–∞ —Å–æ–æ–±—â–µ–Ω–∏—è Telegram –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –∏–ª–∏ —Å—Ç–∞—Ç—å—é?\n"
        "–û—Ç–≤–µ—Ç—å—Ç–µ –¥–∞ –∏–ª–∏ –Ω–µ—Ç."
    )
    user_content = f"Message 1:\n{msg_a.text}\n\nMessage 2:\n{msg_b.text}"
    
    answer = await call_openai(system_prompt, user_content, max_tokens=1)
    return answer.lower().startswith("y")


async def is_message_covered_in_summaries(msg: MessageInfo) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –±—ã–ª–∞ –ª–∏ —Ç–µ–º–∞ —Å–æ–æ–±—â–µ–Ω–∏—è —É–∂–µ –æ—Å–≤–µ—â–µ–Ω–∞ –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–∞–º–º–∞—Ä–∏."""
    recent_context = get_recent_summaries_context(days=7)
    if not recent_context:
        return False
    
    system_prompt = (
        "–û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ, –±—ã–ª–∞ –ª–∏ —Ç–µ–º–∞ –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è Telegram —É–∂–µ –æ—Å–≤–µ—â–µ–Ω–∞ –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–∞–º–º–∞—Ä–∏.\n"
        "–£—á–∏—Ç—ã–≤–∞–π—Ç–µ –∫–∞–∫ –ø—Ä—è–º—ã–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è, —Ç–∞–∫ –∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ–º—ã.\n"
        "–£—á–∏—Ç—ã–≤–∞–π —Å–∏–Ω–æ–Ω–∏–º–∏—á–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è, –Ω–∞–ø—Ä–∏–º–µ—Ä, —Ç–µ–∫—Å—Ç—ã \n"
        "–∞) –ë—ã–≤—à–∏–π —Å–æ—Ç—Ä—É–¥–Ω–∏–∫ OpenAI, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∫–∏–Ω—É–ª —Å—Ç–∞—Ä—Ç–∞–ø –ø–∞—Ä—É –Ω–µ–¥–µ–ª—å –Ω–∞–∑–∞–¥, –Ω–∞–ø–∏—Å–∞–ª –æ–≥—Ä–æ–º–Ω—ã–π "
        "–±–ª–æ–≥-–ø–æ—Å—Ç –æ —Ç–æ–º, –∫–∞–∫–æ–≤–æ —Ç–∞–º —Ä–∞–±–æ—Ç–∞—Ç—å \n"
        "–±) –ü–æ—Ç—Ä—è—Å–∞—é—â–µ–µ —á—Ç–∏–≤–æ –æ—Ç –∏–Ω–∂–µ–Ω–µ—Ä–∞, –∫–æ—Ç–æ—Ä—ã–π –Ω–µ–¥–∞–≤–Ω–æ –ø–æ–∫–∏–Ω—É–ª OpenAI. "
        "–ü–∏—à–µ—Ç –ø—Ä–æ –∫—É–ª—å—Ç—É—Ä—É, –∞—Ç–º–æ—Å—Ñ–µ—Ä—É, –∏ –∫–∞–∫ –≤—Å—ë —Ä–∞–±–æ—Ç–∞–µ—Ç. \n"
        "–≥–æ–≤–æ—Ä—è—Ç –æ–± –æ–¥–Ω–æ–º –∏ —Ç–æ–º –∂–µ, –Ω–æ –Ω–∞–ø–∏—Å–∞–Ω—ã –ø–æ-—Ä–∞–∑–Ω–æ–º—É. \n"
        "–°–º–æ—Ç—Ä–∏ –Ω–∞ –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏ –≤ —Å–æ–æ–±—â–µ–Ω–∏—è—Ö, –µ—Å–ª–∏ –æ–Ω–∏ –≤–µ–¥—É—Ç –Ω–∞ –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ —Ä–µ—Å—É—Ä—Å, —Ç–æ —Å –≤—ã—Å–æ–∫–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é "
        "—ç—Ç–æ –æ–¥–Ω–∞ –∏ —Ç–∞ –∂–µ —Ç–µ–º–∞. \n"
        "–û—Ç–≤–µ—Ç—å—Ç–µ '–¥–∞' –µ—Å–ª–∏ —Ç–µ–º–∞ —É–∂–µ –±—ã–ª–∞ –æ—Å–≤–µ—â–µ–Ω–∞, '–Ω–µ—Ç' –µ—Å–ª–∏ —ç—Ç–æ –Ω–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è."
    )
    
    user_content = f"–ü—Ä–µ–¥—ã–¥—É—â–∏–µ —Å–∞–º–º–∞—Ä–∏:\n{recent_context}\n\n–ù–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ:\n{msg.text}"
    
    try:
        answer = await call_openai(system_prompt, user_content, max_tokens=5)
        return answer.lower().strip().startswith('–¥–∞')
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –ø–æ–∫—Ä—ã—Ç–∏—è –≤ —Å–∞–º–º–∞—Ä–∏: {e}")
        return False


async def is_nlp_related(text: str) -> bool:
    """Use the LLM to decide if a message is NLP related and not advertising."""
    system_prompt = (
        "–í—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä. –û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º –¥–ª—è NLP/ML/AI/programming/Python –¥–∞–π–¥–∂–µ—Å—Ç–∞.\n\n"
        "–ü–†–ò–ù–ò–ú–ê–ô–¢–ï (–æ—Ç–≤–µ—Ç—å—Ç–µ '–¥–∞'):\n"
        "- –ù–∞—É—á–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è\n"
        "- –ù–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –±–∏–±–ª–∏–æ—Ç–µ–∫–∏, –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã\n"
        "- –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –æ–±–∑–æ—Ä—ã –∏ –±–µ–Ω—á–º–∞—Ä–∫–∏\n"
        "- –ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏ –∏ –≤–æ—Ä–∫—à–æ–ø—ã\n"
        "- –û—Ç–∫—Ä—ã—Ç—ã–µ –ø—Ä–æ–µ–∫—Ç—ã –∏ –¥–∞—Ç–∞—Å–µ—Ç—ã\n"
        "- –ù–æ–≤–æ—Å—Ç–∏ –ø—Ä–æ Kaggle, —Ö–∞–∫–∞—Ç–æ–Ω—ã –∏ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è\n"
        "- –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏ —Å–æ–≤–µ—Ç—ã –æ –∫–∞—Ä—å–µ—Ä–µ –∏ –≤–∞–∫–∞–Ω—Å–∏—è—Ö\n"
        "- –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–µ—Ç-–ø—Ä–æ–µ–∫—Ç–∞—Ö\n"
        "- –¢–µ–∫—Å—Ç—ã –ø–æ –∏–∏ –∏ bigtech –∫–æ–º–ø–∞–Ω–∏–∏\n"
        "- –¢–µ–∫—Å—Ç—ã –ø—Ä–æ —Å—Ç–∞—Ä—Ç–∞–ø—ã\n"
        "- –¢–µ–∫—Å—Ç—ã –ø—Ä–æ –∏–∏-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ –∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è (ChatGPT, Claude, Gemini, Grok, DeepSeek, KlingAi, Midjourney etc.)\n"
        "- –õ—é–±—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –ø—Ä–æ –°—ç–º–∞ –ê–ª—å—Ç–º–∞–Ω–∞ (Sam Altman), "
        "OpenAI, Anthropic, Google, Meta, Microsoft, Nvidia, FAANG –∏ —Ç.–¥.\n"
        "- –ù–æ–≤–æ—Å—Ç–∏ –ø—Ä–æ –ø–æ–∫—É–ø–∫—É, –ø—Ä–æ–¥–∞–∂—É –∫–æ–º–ø–∞–Ω–∏–π –∏ –ø–æ–≥–ª–æ—â–µ–Ω–∏—è\n:"
        "- –ù–æ–≤–æ—Å—Ç–∏ –ø—Ä–æ —É–≤–æ–ª—å–Ω–µ–Ω–∏—è –∏ –ò–ò-—ç–∫–æ–Ω–æ–º–∏–∫—É\n:"
        "- –ù–æ–≤–æ—Å—Ç–∏ –ø—Ä–æ GPU –∏ –∂–µ–ª–µ–∑–æ\n:"
        "- –ù–æ–≤–æ—Å—Ç–∏ –ø—Ä–æ –≤–∞–π–±–∫–æ–¥–∏–Ω–≥ (vibe coding) –∏ –∏–∏-–∞–≥–µ–Ω—Ç–æ–≤ (—á–∞—Å—Ç–æ –ø—Ä–æ—Å—Ç–æ –∞–≥–µ–Ω—Ç—ã, agents)\n:"
        "- –ù–æ–≤–æ—Å—Ç–∏ –ø—Ä–æ LLM –∏ –∏—Ö –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ\n:"
        "- –ù–æ–≤–æ—Å—Ç–∏ –ø—Ä–æ –æ–ø—ã—Ç —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è\n:"
        "- –ú–µ–º—ã –∏–∑ –º–∏—Ä–∞ –∏–∏ –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π\n:"

        "–û–¢–ö–õ–û–ù–Ø–ô–¢–ï (–æ—Ç–≤–µ—Ç—å—Ç–µ '–Ω–µ—Ç'):\n"
        "- –ö—É—Ä—Å—ã, –æ–±—É—á–µ–Ω–∏–µ, –ø–ª–∞—Ç–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã\n"
        "- –†–µ–∫–ª–∞–º–∞ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö LLM (GigaChat, YandexGPT)\n"
        "- –ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ —É—Å–ª—É–≥–∏\n"
        "- –í–µ–±–∏–Ω–∞—Ä—ã —Å –ø—Ä–æ–¥–∞–∂–∞–º–∏\n"
        "- –ú–∞—Å—Ç–µ—Ä-–∫–ª–∞—Å—Å—ã —Å —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–∞–º–∏\n"
        "- Hiring days\n"
        "- –í —Å–æ–æ–±—â–µ–Ω–∏–∏ –µ—Å—Ç—å —Å—Å—ã–ª–∫–∏ –≤–µ–¥—É—â–∏–µ –Ω–∞ –±–æ—Ç–æ–≤, –ø—Ä–∏ —ç—Ç–æ–º –Ω–µ —É–∫–∞–∑–∞–Ω–æ –ø–µ—Ä–µ–¥ —Å—Å—ã–ª–∫–æ–π, —á—Ç–æ —ç—Ç–æ —Å—Å—ã–ª–∫–∞ –Ω–∞ –±–æ—Ç–∞\n"
        "–û—Ç–≤–µ—á–∞–π—Ç–µ —Ç–æ–ª—å–∫–æ '–¥–∞' –∏–ª–∏ '–Ω–µ—Ç'."
    )
    
    answer = await call_openai(system_prompt, text, max_tokens=5)
    return answer.lower().strip().startswith('–¥–∞')


async def summarize_text(messages: List[MessageInfo]) -> str:
    """Call LLM to summarize the given messages with links."""
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –Ω–æ–º–µ—Ä–æ–≤ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    messages_with_sources = []
    total_original_length = 0
    
    for i, msg in enumerate(messages, 1):
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å–æ–æ–±—â–µ–Ω–∏—è
        links = extract_links(msg.text)
        source_info = f"[{i}] {msg.text}"
        if links:
            source_info += f" (–°—Å—ã–ª–∫–∏: {', '.join(links)})"
        messages_with_sources.append(source_info)
        total_original_length += count_characters(msg.text)
    
    messages_text = "\n\n".join(messages_with_sources)
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É —Å–∞–º–º–∞—Ä–∏
    max_summary_length = min(total_original_length * 2, 16000)
    
    system_prompt = (
        "–û–±–æ–±—â–∏—Ç–µ —Å–ª–µ–¥—É—é—â–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è Telegram –≤ –∫—Ä–∞—Ç–∫–∏–π –µ–∂–µ–¥–Ω–µ–≤–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç, —Å—Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ NLP. "
        "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π—Ç–µ –¥–∞–π–¥–∂–µ—Å—Ç –ø–æ —Ç–µ–º–∞–º –∏–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –±–ª–æ–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —É–∫–∞–∑—ã–≤–∞–π—Ç–µ "
        "–Ω–æ–º–µ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –≤ –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã—Ö —Å–∫–æ–±–∫–∞—Ö [1], [2], [3] –∏ —Ç.–¥. –ï—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –≥–æ–≤–æ—Ä—è—Ç "
        "–æ–± –æ–¥–Ω–æ–º –∏ —Ç–æ–º –∂–µ, —Ç–æ —É–∫–∞–∑—ã–≤–∞–π—Ç–µ –≤—Å–µ. "
        "–î–ª—è —Å—Ç–∞—Ç–µ–π –∏ –Ω–æ–≤—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –∫—Ä–∞—Ç–∫–æ –æ–ø–∏—à–∏—Ç–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã: –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã. "
        "–í–ê–ñ–ù–û: –ù–ï –¥–æ–±–∞–≤–ª—è–π—Ç–µ –≤–≤–µ–¥–µ–Ω–∏–µ –∏ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ. –ù–∞—á–∏–Ω–∞–π—Ç–µ —Å—Ä–∞–∑—É —Å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –¥–∞–π–¥–∂–µ—Å—Ç–∞. "
        "–í–ê–ñ–ù–û: –£—á–∏—Ç—ã–≤–∞–π—Ç–µ, —á—Ç–æ Telegram-–∫–∞–Ω–∞–ª—ã –≤—ã—Ä–∞–∂–∞—é—Ç —Å–≤–æ–µ –º–Ω–µ–Ω–∏–µ –∏ –º–æ–≥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—É–±—ä–µ–∫—Ç–∏–≤–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏. "
        "–ï—Å–ª–∏ –∞–≤—Ç–æ—Ä –∫–∞–Ω–∞–ª–∞ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç —á—Ç–æ-—Ç–æ –∏–ª–∏ –¥–∞–µ—Ç —Å–æ–≤–µ—Ç—ã, –ù–ï –¥–æ—Å–ª–æ–≤–Ω–æ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–π—Ç–µ —ç—Ç–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏. "
        "–í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ –æ–ø–∏—à–∏—Ç–µ —Ñ–∞–∫—Ç—ã –∏ —Å–æ–±—ã—Ç–∏—è, –æ –∫–æ—Ç–æ—Ä—ã—Ö –∏–¥–µ—Ç —Ä–µ—á—å. "
        "–í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å—Ç–∏–ª—å –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º —Å–æ–æ–±—â–µ–Ω–∏—è–º –∏–∑ Telegram-–∫–∞–Ω–∞–ª–æ–≤. "
        "–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Å—Ç–∏–ª—å –∏—Å—Ö–æ–¥–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏ —Å–æ–∑–¥–∞–≤–∞–π—Ç–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≤ —Ç–æ–º –∂–µ –¥—É—Ö–µ - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —ç–º–æ–¥–∑–∏, "
        "–∫–æ—Ä–æ—Ç–∫–∏–µ –∏ —è—Ä–∫–∏–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ –¥–ª—è Telegram. –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–µ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏. "
        "–í–ê–ñ–ù–û: –í –Ω–∞—á–∞–ª–µ –∫–∞–∂–¥–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –¥–æ–±–∞–≤–ª—è–π—Ç–µ 1-5 —ç–º–æ–¥–∑–∏, –∫–æ—Ç–æ—Ä—ã–µ –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ –∏ –º–µ—Ç–∞—Ñ–æ—Ä–∏—á–Ω–æ "
        "–æ–ø–∏—Å—ã–≤–∞—é—Ç —Å—É—Ç—å –Ω–æ–≤–æ—Å—Ç–∏. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–µ, –Ω–æ –ø–æ–Ω—è—Ç–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —ç–º–æ–¥–∑–∏. "
        "–≠–º–æ–¥–∑–∏ –ù–ï –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–∏–Ω–æ–Ω–∏–º–∏—á–Ω—ã–º–∏. "
        "–ü—Ä–∏–º–µ—Ä—ã —Å—Ç–∏–ª—è: '<b>üß†üíß –ü–µ—Ä–µ—Ö–æ–¥—ã –º–µ–∂–¥—É AI-–∫–æ–º–ø–∞–Ω–∏—è–º–∏</b>', "
        "'<b>ü§ñüé≠üëÖ –ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π</b>', "
        "'<b>ü´±üëÅÔ∏èü´≤ –ü—Ä–æ—Ä—ã–≤ –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏</b>', "
        "'<b>üí°üëÖ –ò–Ω–Ω–æ–≤–∞—Ü–∏–∏ –≤ NLP</b>', "
        "'<b>üéØü™øüìà –ù–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ Kaggle</b>', "
        "'<b>üíºüå™Ô∏èüìâ –†—ã–Ω–æ–∫ AI-–≤–∞–∫–∞–Ω—Å–∏–π</b>'. "
        "–í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¢–û–õ–¨–ö–û HTML-—Ç–µ–≥–∏ <b>—Ç–µ–∫—Å—Ç</b> –¥–ª—è –∂–∏—Ä–Ω–æ–≥–æ —à—Ä–∏—Ñ—Ç–∞, "
        "–ù–ï –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ **—Ç–µ–∫—Å—Ç** –∏–ª–∏ –¥—Ä—É–≥–∏–µ Markdown —Ä–∞–∑–º–µ—Ç–∫–∏. "
        "–í–ê–ñ–ù–û: –ù–ï —Å–æ–∑–¥–∞–≤–∞–π—Ç–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –≤ —Ç–µ–∫—Å—Ç–µ, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ–ª—å–∫–æ –Ω–æ–º–µ—Ä–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ [1], [2], [3] –∏ —Ç.–¥. "
        "–°—Å—ã–ª–∫–∏ –±—É–¥—É—Ç –¥–æ–±–∞–≤–ª–µ–Ω—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏. "
        f"–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–∞–º–º–∞—Ä–∏ –ù–ï –î–û–õ–ñ–ù–ê –ø—Ä–µ–≤—ã—à–∞—Ç—å {max_summary_length} —Å–∏–º–≤–æ–ª–æ–≤. "
        "–î–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π —Å–æ–∑–¥–∞–≤–∞–π—Ç–µ —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–µ —Å–∞–º–º–∞—Ä–∏, "
        "–Ω–æ —Å—Ç—Ä–æ–≥–æ —Å–æ–±–ª—é–¥–∞–π—Ç–µ –ª–∏–º–∏—Ç —Å–∏–º–≤–æ–ª–æ–≤."
        "denissexy - —ç—Ç–æ –∫–∞–Ω–∞–ª –ø—Ä–æ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç"
    )
    
    # –í—ã—á–∏—Å–ª—è–µ–º max_tokens –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã —Å–∞–º–º–∞—Ä–∏ (–ø—Ä–∏–º–µ—Ä–Ω–æ 4 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω)
    max_tokens = 16000
    
    result = await call_openai(system_prompt, messages_text, max_tokens=max_tokens)
    if not result:
        return "–û—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ–±–æ–±—â–µ–Ω–∏–µ"
    
    print(f"–î–ª–∏–Ω–∞ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: {total_original_length} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"–î–ª–∏–Ω–∞ —Å–∞–º–º–∞—Ä–∏: {count_characters(result)} —Å–∏–º–≤–æ–ª–æ–≤")
    
    # –ó–∞–º–µ–Ω—è–µ–º –Ω–æ–º–µ—Ä–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–∞ HTML-—Å—Å—ã–ª–∫–∏
    def replace_source_with_links(match):
        content = match.group(1)  # —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤–Ω—É—Ç—Ä–∏ —Å–∫–æ–±–æ–∫
        numbers = [num.strip() for num in content.split(',')]
        source_links = []
        
        for num_str in numbers:
            try:
                num = int(num_str)
                if 1 <= num <= len(messages):
                    msg = messages[num-1]
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å–æ–æ–±—â–µ–Ω–∏—è
                    links = extract_links(msg.text)
                    telegram_link = msg.get_telegram_link()
                    
                    # –í—Å–µ–≥–¥–∞ —Å–æ–∑–¥–∞–µ–º –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—É –∫–∞–Ω–∞–ª–∞
                    channel_abbr = create_channel_abbreviation(msg.channel)
                    
                    if links:
                        # –ï—Å–ª–∏ –µ—Å—Ç—å –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏, –¥–æ–±–∞–≤–ª—è–µ–º –∏ –≤–Ω–µ—à–Ω—é—é —Å—Å—ã–ª–∫—É, –∏ —Å—Å—ã–ª–∫—É –Ω–∞ Telegram-–ø–æ—Å—Ç
                        main_link = links[0]
                        source_links.append(
                            f'<a href="{main_link}">[{num}]</a> <a href="{telegram_link}">[{channel_abbr}]</a>'
                        )
                    else:
                        # –ï—Å–ª–∏ –≤–Ω–µ—à–Ω–∏—Ö —Å—Å—ã–ª–æ–∫ –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Å—Å—ã–ª–∫—É –Ω–∞ Telegram-—Å–æ–æ–±—â–µ–Ω–∏–µ —Å –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–æ–π
                        source_links.append(f'<a href="{telegram_link}">[{channel_abbr}]</a>')
                        
            except ValueError:
                continue
        return ', '.join(source_links)

    # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ [1], [1,2], [1,2,3] –∏ —Ç.–¥.
    all_sources_pattern = r'\[(\d+(?:,\s*\d+)*)\]'
    result = re.sub(all_sources_pattern, replace_source_with_links, result)

    return result


async def fetch_messages():
    """Fetch messages from source channels in the last 24 hours."""
    since = datetime.now(timezone.utc) - timedelta(days=1)
    all_msgs = []
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
    processed_messages = load_summarization_history()
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(processed_messages)} —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏")
    
    for channel in SOURCE_CHANNELS:
        print(f"Fetching messages from {channel}...")
        channel_msgs = []
        async for msg in user_client.iter_messages(channel, offset_date=None, min_id=0, reverse=False):
            if msg.date < since:
                break
            if msg.message:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å–æ–æ–±—â–µ–Ω–∏—è
                links = extract_links(msg.message)
                main_link = links[0] if links else ""
                
                message_info = MessageInfo(
                    text=msg.message,
                    channel=channel,
                    message_id=msg.id,
                    date=msg.date,
                    link=main_link
                )
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –±—ã–ª–æ –ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ
                if not is_message_processed(message_info, processed_messages):
                    channel_msgs.append(message_info)
                    all_msgs.append(message_info)
                else:
                    print(f"  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ {msg.id} –∏–∑ {channel}")
                    
        print(f"  Found {len(channel_msgs)} –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π from {channel}")
    return all_msgs


async def remove_duplicates(messages: List[MessageInfo]) -> List[MessageInfo]:
    unique_msgs: List[MessageInfo] = []
    seen_links = set()
    
    for msg in messages:
        links = extract_links(msg.text)
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ —Å—Å—ã–ª–∫–∞–º - –µ—Å–ª–∏ —Å—Å—ã–ª–∫–∞ —É–∂–µ –±—ã–ª–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        if links and any(link in seen_links for link in links):
            print(f"  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç –ø–æ —Å—Å—ã–ª–∫–µ: {links[0]}")
            continue
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ —Ç–µ–∫—Å—Ç—É
        duplicate = False
        for u in unique_msgs:
            if SequenceMatcher(None, msg.text, u.text).ratio() > SIMILARITY_THRESHOLD:
                print(f"  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç –ø–æ —Ç–µ–∫—Å—Ç—É: {msg.text[:50]}...")
                duplicate = True
                break
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –¥—É–±–ª–∏–∫–∞—Ç –ø–æ —Ç–µ–∫—Å—Ç—É, –ø—Ä–æ–≤–µ—Ä—è–µ–º —á–µ—Ä–µ–∑ LLM
        if not duplicate:
            for u in unique_msgs:
                try:
                    if await are_messages_duplicate(msg, u):
                        print(f"  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç –ø–æ LLM: {msg.text[:50]}...")
                        duplicate = True
                        break
                except Exception as e:
                    print(f"  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –¥—É–±–ª–∏–∫–∞—Ç–∞ —á–µ—Ä–µ–∑ LLM: {e}")
                    # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ LLM, —Å—á–∏—Ç–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è —Ä–∞–∑–Ω—ã–º–∏
                    continue
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –±—ã–ª–∞ –ª–∏ —Ç–µ–º–∞ —É–∂–µ –æ—Å–≤–µ—â–µ–Ω–∞ –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–∞–º–º–∞—Ä–∏
        if not duplicate and ENABLE_SUMMARIES_DEDUPLICATION:
            try:
                if await is_message_covered_in_summaries(msg):
                    print(f"  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ, —É–∂–µ –æ—Å–≤–µ—â–µ–Ω–Ω–æ–µ –≤ —Å–∞–º–º–∞—Ä–∏: {msg.text[:50]}...")
                    duplicate = True
            except Exception as e:
                print(f"  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –ø–æ–∫—Ä—ã—Ç–∏—è –≤ —Å–∞–º–º–∞—Ä–∏: {e}")
                # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏, —Å—á–∏—Ç–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –Ω–æ–≤—ã–º
                pass
        
        if not duplicate:
            unique_msgs.append(msg)
            seen_links.update(links)
            print(f"  –î–æ–±–∞–≤–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ: {msg.text[:50]}...")
    
    return unique_msgs


async def main():
    # Start both clients
    await user_client.start()
    await bot_client.start(bot_token=BOT_TOKEN)
    
    try:
        messages = await fetch_messages()
        print(f"Fetched {len(messages)} new messages")
        
        if not messages:
            print("No new messages found")
            return
        
        # Apply NLP filtering to remove advertising
        filtered = []
        all_checked_messages = []  # –í—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã —á–µ—Ä–µ–∑ is_nlp_related
        
        for i, msg in enumerate(messages):
            print(f"Checking message {i+1}/{len(messages)}...")
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            all_checked_messages.append(msg)
            
            if await is_nlp_related(msg.text):
                filtered.append(msg)
                print(f"  ‚úì Message {i+1} is NLP-related: {msg.text[:100]}; {msg.link}")
            else:
                print(f"  ‚úó Message {i+1} is not NLP-related (likely advertising): {msg.text[:100]}; {msg.link}")
        print(f"{len(filtered)} messages after NLP filter")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –∏—Å—Ç–æ—Ä–∏—é
        save_summarization_history(all_checked_messages)
        print(f"Saved {len(all_checked_messages)} checked messages to history")
        
        if not filtered:
            print("No new NLP-related messages found")
            return
            
        unique = await remove_duplicates(filtered)
        print(f"{len(unique)} messages after deduplication")
        
        if not unique:
            print("No unique NLP messages found")
            return
            
        summary = await summarize_text(unique)
        await user_client.send_message(TARGET_CHANNEL, summary, parse_mode='html')
        print("Summary sent")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∞–º–º–∞—Ä–∏ –≤ –∏—Å—Ç–æ—Ä–∏—é
        channels = list(set(msg.channel for msg in unique))
        summary_info = SummaryInfo(
            content=summary,
            date=datetime.now(timezone.utc),
            message_count=len(unique),
            channels=channels
        )
        save_summary_to_history(summary_info)
        print(f"Summary saved to history (channels: {channels})")
        
    finally:
        # Disconnect both clients
        await user_client.disconnect()
        await bot_client.disconnect()


if __name__ == "__main__":
    asyncio.run(main())
