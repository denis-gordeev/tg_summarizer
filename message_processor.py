import random
import re
from typing import List, Set
from difflib import SequenceMatcher
from models import MessageInfo, SummaryInfo
from utils import call_openai, extract_links, count_characters
from config import SIMILARITY_THRESHOLD, ENABLE_SUMMARIES_DEDUPLICATION
from history_manager import (
    get_recent_summaries_context, get_recent_group_summaries_context
)
from channel_manager import (
    load_discovered_channels, load_similar_channels, load_banned_channels,
    create_channel_abbreviation
)
from prompts import prompts


def get_all_source_channels() -> List[str]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∫–∞–Ω–∞–ª–æ–≤ –∏–∑ .env, –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö –∏ –ø–æ—Ö–æ–∂–∏—Ö –∫–∞–Ω–∞–ª–æ–≤."""
    from config import SOURCE_CHANNELS
    
    discovered_channels = set(load_discovered_channels())
    similar_channels = set(load_similar_channels())
    banned_channels = set(load_banned_channels())
    
    # –ò—Å–∫–ª—é—á–∞–µ–º –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–∞–Ω–∞–ª—ã
    all_channels_set = SOURCE_CHANNELS | discovered_channels | similar_channels
    all_channels_set = all_channels_set - banned_channels
    
    all_channels = list(all_channels_set)
    random.shuffle(all_channels)
    print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º {len(SOURCE_CHANNELS)} –∫–∞–Ω–∞–ª–æ–≤ –∏–∑ .env, "
          f"{len(discovered_channels)} –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö, {len(similar_channels)} –ø–æ—Ö–æ–∂–∏—Ö –∫–∞–Ω–∞–ª–æ–≤ "
          f"(–∏—Å–∫–ª—é—á–µ–Ω–æ {len(banned_channels)} –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö)")
    return all_channels


def is_message_processed(msg: MessageInfo, processed_messages: Set[str]) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –±—ã–ª–æ –ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ä–∞–Ω–µ–µ."""
    msg_id = f"{msg.channel}_{msg.message_id}_{hash(msg.text)}"
    return msg_id in processed_messages


async def are_messages_duplicate(msg_a: MessageInfo, msg_b: MessageInfo) -> bool:
    """Use the LLM to see if two messages cover the same topic."""
    user_content = f"Message 1:\n{msg_a.text}\n\nMessage 2:\n{msg_b.text}"
    
    answer = await call_openai(prompts.DUPLICATE_CHECK_PROMPT, user_content, max_tokens=1)
    return answer.lower().startswith("y")


async def is_message_covered_in_summaries(msg: MessageInfo) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –±—ã–ª–∞ –ª–∏ —Ç–µ–º–∞ —Å–æ–æ–±—â–µ–Ω–∏—è —É–∂–µ –æ—Å–≤–µ—â–µ–Ω–∞ –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–∞–º–º–∞—Ä–∏."""
    if not ENABLE_SUMMARIES_DEDUPLICATION:
        return False
    
    recent_summaries = get_recent_summaries_context()
    if not recent_summaries:
        return False
    
    user_content = f"""–ü—Ä–µ–¥—ã–¥—É—â–∏–µ –¥–∞–π–¥–∂–µ—Å—Ç—ã:
        {recent_summaries}

        –ù–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ:
        {msg.text}

        –ë—ã–ª–∞ –ª–∏ —ç—Ç–∞ —Ç–µ–º–∞ —É–∂–µ –æ—Å–≤–µ—â–µ–Ω–∞ –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –¥–∞–π–¥–∂–µ—Å—Ç–∞—Ö?"""

    try:
        result = await call_openai(prompts.SUMMARY_COVERAGE_CHECK_PROMPT, user_content, max_tokens=10)
        return result.strip().upper() == "–î–ê"
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –ø–æ–∫—Ä—ã—Ç–∏—è –≤ —Å–∞–º–º–∞—Ä–∏: {e}")
        return False


async def is_message_covered_in_group_summaries(msg: MessageInfo) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –±—ã–ª–∞ –ª–∏ —Ç–µ–º–∞ —Å–æ–æ–±—â–µ–Ω–∏—è —É–∂–µ –æ—Å–≤–µ—â–µ–Ω–∞ –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–∞–º–º–∞—Ä–∏ –≥—Ä—É–ø–ø."""
    if not ENABLE_SUMMARIES_DEDUPLICATION:
        return False
    
    recent_summaries = get_recent_group_summaries_context()
    if not recent_summaries:
        return False
    
    user_content = f"""–ü—Ä–µ–¥—ã–¥—É—â–∏–µ –¥–∞–π–¥–∂–µ—Å—Ç—ã –≥—Ä—É–ø–ø:
{recent_summaries}

–ù–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ:
{msg.text}

–ë—ã–ª–∞ –ª–∏ —ç—Ç–∞ —Ç–µ–º–∞ —É–∂–µ –æ—Å–≤–µ—â–µ–Ω–∞ –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –¥–∞–π–¥–∂–µ—Å—Ç–∞—Ö –≥—Ä—É–ø–ø?"""

    try:
        result = await call_openai(prompts.GROUP_SUMMARY_COVERAGE_CHECK_PROMPT, user_content, max_tokens=10)
        return result.strip().upper() == "–î–ê"
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –ø–æ–∫—Ä—ã—Ç–∏—è –≤ —Å–∞–º–º–∞—Ä–∏ –≥—Ä—É–ø–ø: {e}")
        return False


async def is_nlp_related(text: str) -> bool:
    """Use the LLM to decide if a message is NLP related and not advertising."""
    answer = await call_openai(prompts.NLP_RELEVANCE_PROMPT, text, max_tokens=5)
    return answer.lower().strip().startswith('–¥–∞')


async def remove_duplicates(messages: List[MessageInfo]) -> List[MessageInfo]:
    unique_msgs: List[MessageInfo] = []
    seen_links = set()
    
    for msg in messages:
        links = extract_links(msg.text)
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ —Å—Å—ã–ª–∫–∞–º - –µ—Å–ª–∏ —Å—Å—ã–ª–∫–∞ —É–∂–µ –±—ã–ª–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        if links and any(link in seen_links for link in links):
            print(f"  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç –ø–æ —Å—Å—ã–ª–∫–µ: {links[0]}")
            continue
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ —Ç–µ–∫—Å—Ç—É
        duplicate = False
        for u in unique_msgs:
            if SequenceMatcher(None, msg.text, u.text).ratio() > SIMILARITY_THRESHOLD:
                print(f"  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç –ø–æ —Ç–µ–∫—Å—Ç—É: {msg.text[:50]}...")
                duplicate = True
                break
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –¥—É–±–ª–∏–∫–∞—Ç –ø–æ —Ç–µ–∫—Å—Ç—É, –ø—Ä–æ–≤–µ—Ä—è–µ–º —á–µ—Ä–µ–∑ LLM
        if not duplicate:
            for u in unique_msgs:
                try:
                    if await are_messages_duplicate(msg, u):
                        print(f"  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç –ø–æ LLM: {msg.text[:50]}...")
                        duplicate = True
                        break
                except Exception as e:
                    print(f"  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –¥—É–±–ª–∏–∫–∞—Ç–∞ —á–µ—Ä–µ–∑ LLM: {e}")
                    # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ LLM, —Å—á–∏—Ç–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è —Ä–∞–∑–Ω—ã–º–∏
                    continue
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –±—ã–ª–∞ –ª–∏ —Ç–µ–º–∞ —É–∂–µ –æ—Å–≤–µ—â–µ–Ω–∞ –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–∞–º–º–∞—Ä–∏
        if not duplicate and ENABLE_SUMMARIES_DEDUPLICATION:
            try:
                if await is_message_covered_in_summaries(msg):
                    print(f"  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ, —É–∂–µ –æ—Å–≤–µ—â–µ–Ω–Ω–æ–µ –≤ —Å–∞–º–º–∞—Ä–∏: {msg.text[:50]}...")
                    duplicate = True
            except Exception as e:
                print(f"  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –ø–æ–∫—Ä—ã—Ç–∏—è –≤ —Å–∞–º–º–∞—Ä–∏: {e}")
                # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏, —Å—á–∏—Ç–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –Ω–æ–≤—ã–º
                pass
        
        if not duplicate:
            unique_msgs.append(msg)
            seen_links.update(links)
            print(f"  –î–æ–±–∞–≤–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ: {msg.text[:50]}...")
    
    return unique_msgs


async def remove_group_duplicates(messages: List[MessageInfo]) -> List[MessageInfo]:
    """Remove duplicates from group messages, checking against group summaries history."""
    unique_msgs: List[MessageInfo] = []
    seen_links = set()
    
    for msg in messages:
        links = extract_links(msg.text)
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ —Å—Å—ã–ª–∫–∞–º - –µ—Å–ª–∏ —Å—Å—ã–ª–∫–∞ —É–∂–µ –±—ã–ª–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        if links and any(link in seen_links for link in links):
            print(f"  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç –ø–æ —Å—Å—ã–ª–∫–µ: {links[0]}")
            continue
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ —Ç–µ–∫—Å—Ç—É
        duplicate = False
        for u in unique_msgs:
            if SequenceMatcher(None, msg.text, u.text).ratio() > SIMILARITY_THRESHOLD:
                print(f"  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç –ø–æ —Ç–µ–∫—Å—Ç—É: {msg.text[:50]}...")
                duplicate = True
                break
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –¥—É–±–ª–∏–∫–∞—Ç –ø–æ —Ç–µ–∫—Å—Ç—É, –ø—Ä–æ–≤–µ—Ä—è–µ–º —á–µ—Ä–µ–∑ LLM
        if not duplicate:
            for u in unique_msgs:
                try:
                    if await are_messages_duplicate(msg, u):
                        print(f"  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç –ø–æ LLM: {msg.text[:50]}...")
                        duplicate = True
                        break
                except Exception as e:
                    print(f"  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –¥—É–±–ª–∏–∫–∞—Ç–∞ —á–µ—Ä–µ–∑ LLM: {e}")
                    # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ LLM, —Å—á–∏—Ç–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è —Ä–∞–∑–Ω—ã–º–∏
                    continue
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –±—ã–ª–∞ –ª–∏ —Ç–µ–º–∞ —É–∂–µ –æ—Å–≤–µ—â–µ–Ω–∞ –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–∞–º–º–∞—Ä–∏ –≥—Ä—É–ø–ø
        if not duplicate and ENABLE_SUMMARIES_DEDUPLICATION:
            try:
                if await is_message_covered_in_group_summaries(msg):
                    print(f"  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ, —É–∂–µ –æ—Å–≤–µ—â–µ–Ω–Ω–æ–µ –≤ —Å–∞–º–º–∞—Ä–∏ –≥—Ä—É–ø–ø: "
                          f"{msg.text[:50]}...")
                    duplicate = True
            except Exception as e:
                print(f"  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –ø–æ–∫—Ä—ã—Ç–∏—è –≤ —Å–∞–º–º–∞—Ä–∏ –≥—Ä—É–ø–ø: {e}")
                # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏, —Å—á–∏—Ç–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –Ω–æ–≤—ã–º
                pass
        
        if not duplicate:
            unique_msgs.append(msg)
            seen_links.update(links)
            print(f"  –î–æ–±–∞–≤–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –∏–∑ –≥—Ä—É–ø–ø—ã: {msg.text[:50]}...")
    
    return unique_msgs


async def summarize_text(messages: List[MessageInfo]) -> str:
    """Call LLM to summarize the given messages with links."""
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –Ω–æ–º–µ—Ä–æ–≤ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    messages_with_sources = []
    total_original_length = 0
    
    for i, msg in enumerate(messages, 1):
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å–æ–æ–±—â–µ–Ω–∏—è
        links = extract_links(msg.text)
        source_info = f"[{i}] {msg.text}"
        if links:
            source_info += f" (–°—Å—ã–ª–∫–∏: {', '.join(links)})"
        messages_with_sources.append(source_info)
        total_original_length += count_characters(msg.text)
    
    messages_text = "\n\n".join(messages_with_sources)
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É —Å–∞–º–º–∞—Ä–∏
    max_summary_length = min(total_original_length // 3, 50)
    
    system_prompt = prompts.CHANNEL_SUMMARY_PROMPT.format(max_summary_length=max_summary_length)
    
    # –í—ã—á–∏—Å–ª—è–µ–º max_tokens –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã —Å–∞–º–º–∞—Ä–∏ (–ø—Ä–∏–º–µ—Ä–Ω–æ 4 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω)
    max_tokens = 16000
    
    result = await call_openai(system_prompt, messages_text, max_tokens=max_tokens)
    if not result:
        return "–û—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ–±–æ–±—â–µ–Ω–∏–µ"
    
    print(f"–î–ª–∏–Ω–∞ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: {total_original_length} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"–î–ª–∏–Ω–∞ —Å–∞–º–º–∞—Ä–∏: {count_characters(result)} —Å–∏–º–≤–æ–ª–æ–≤")
    
    # –ó–∞–º–µ–Ω—è–µ–º –Ω–æ–º–µ—Ä–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–∞ HTML-—Å—Å—ã–ª–∫–∏
    def replace_source_with_links(match):
        content = match.group(1)  # —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤–Ω—É—Ç—Ä–∏ —Å–∫–æ–±–æ–∫
        numbers = [num.strip() for num in content.split(',')]
        source_links = []
        
        for num_str in numbers:
            try:
                num = int(num_str)
                if 1 <= num <= len(messages):
                    msg = messages[num-1]
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å–æ–æ–±—â–µ–Ω–∏—è
                    links = extract_links(msg.text)
                    telegram_link = msg.get_telegram_link()
                    
                    # –í—Å–µ–≥–¥–∞ —Å–æ–∑–¥–∞–µ–º –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—É –∫–∞–Ω–∞–ª–∞
                    channel_abbr = create_channel_abbreviation(msg.channel)
                    
                    if links:
                        # –ï—Å–ª–∏ –µ—Å—Ç—å –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏, –¥–æ–±–∞–≤–ª—è–µ–º –∏ –≤–Ω–µ—à–Ω—é—é —Å—Å—ã–ª–∫—É, –∏ —Å—Å—ã–ª–∫—É –Ω–∞ Telegram-–ø–æ—Å—Ç
                        main_link = links[0]
                        source_links.append(
                            f'<a href="{main_link}">[{num}]</a> <a href="{telegram_link}">[{channel_abbr}]</a>'
                        )
                    else:
                        # –ï—Å–ª–∏ –≤–Ω–µ—à–Ω–∏—Ö —Å—Å—ã–ª–æ–∫ –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Å—Å—ã–ª–∫—É –Ω–∞ Telegram-—Å–æ–æ–±—â–µ–Ω–∏–µ —Å –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–æ–π
                        source_links.append(f'<a href="{telegram_link}">[{channel_abbr}]</a>')
                        
            except ValueError:
                continue
        return ', '.join(source_links)

    # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ [1], [1,2], [1,2,3] –∏ —Ç.–¥.
    all_sources_pattern = r'\[(\d+(?:,\s*\d+)*)\]'
    result = re.sub(all_sources_pattern, replace_source_with_links, result)
    print("result:", "="*100, "\n", result, "\n", "="*100, "\n")

    return result


async def summarize_group_text(messages: List[MessageInfo]) -> str:
    """Call LLM to summarize group messages with community review header."""
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –Ω–æ–º–µ—Ä–æ–≤ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    messages_with_sources = []
    total_original_length = 0
    
    for i, msg in enumerate(messages, 1):
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å–æ–æ–±—â–µ–Ω–∏—è
        links = extract_links(msg.text)
        source_info = f"[{i}] {msg.text}"
        if links:
            source_info += f" (–°—Å—ã–ª–∫–∏: {', '.join(links)})"
        messages_with_sources.append(source_info)
        total_original_length += count_characters(msg.text)
    
    messages_text = "\n\n".join(messages_with_sources)
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É —Å–∞–º–º–∞—Ä–∏
    max_summary_length = min(total_original_length * 2, 16000)
    
    system_prompt = prompts.GROUP_SUMMARY_PROMPT.format(max_summary_length=max_summary_length)
    
    # –í—ã—á–∏—Å–ª—è–µ–º max_tokens –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã —Å–∞–º–º–∞—Ä–∏ (–ø—Ä–∏–º–µ—Ä–Ω–æ 4 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω)
    max_tokens = 16000
    
    result = await call_openai(system_prompt, messages_text, max_tokens=max_tokens)
    if not result:
        return "–û—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ–±–æ–±—â–µ–Ω–∏–µ"
    
    print(f"–î–ª–∏–Ω–∞ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≥—Ä—É–ø–ø: {total_original_length} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"–î–ª–∏–Ω–∞ —Å–∞–º–º–∞—Ä–∏ –≥—Ä—É–ø–ø: {count_characters(result)} —Å–∏–º–≤–æ–ª–æ–≤")
    
    # –ó–∞–º–µ–Ω—è–µ–º –Ω–æ–º–µ—Ä–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–∞ HTML-—Å—Å—ã–ª–∫–∏
    def replace_source_with_links(match):
        content = match.group(1)  # —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤–Ω—É—Ç—Ä–∏ —Å–∫–æ–±–æ–∫
        numbers = [num.strip() for num in content.split(',')]
        source_links = []
        
        for num_str in numbers:
            try:
                num = int(num_str)
                if 1 <= num <= len(messages):
                    msg = messages[num-1]
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å–æ–æ–±—â–µ–Ω–∏—è
                    links = extract_links(msg.text)
                    telegram_link = msg.get_telegram_link()
                    
                    # –í—Å–µ–≥–¥–∞ —Å–æ–∑–¥–∞–µ–º –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—É –∫–∞–Ω–∞–ª–∞
                    channel_abbr = create_channel_abbreviation(msg.channel)
                    
                    if links:
                        # –ï—Å–ª–∏ –µ—Å—Ç—å –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏, –¥–æ–±–∞–≤–ª—è–µ–º –∏ –≤–Ω–µ—à–Ω—é—é —Å—Å—ã–ª–∫—É, –∏ —Å—Å—ã–ª–∫—É –Ω–∞ Telegram-–ø–æ—Å—Ç
                        main_link = links[0]
                        source_links.append(
                            f'<a href="{main_link}">[{num}]</a> <a href="{telegram_link}">[{channel_abbr}]</a>'
                        )
                    else:
                        # –ï—Å–ª–∏ –≤–Ω–µ—à–Ω–∏—Ö —Å—Å—ã–ª–æ–∫ –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Å—Å—ã–ª–∫—É –Ω–∞ Telegram-—Å–æ–æ–±—â–µ–Ω–∏–µ —Å –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–æ–π
                        source_links.append(f'<a href="{telegram_link}">[{channel_abbr}]</a>')
                        
            except ValueError:
                continue
        return ', '.join(source_links)

    # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ [1], [1,2], [1,2,3] –∏ —Ç.–¥.
    all_sources_pattern = r'\[(\d+(?:,\s*\d+)*)\]'
    result = re.sub(all_sources_pattern, replace_source_with_links, result)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ "–û–±–∑–æ—Ä —Å–æ–æ–±—â–µ—Å—Ç–≤–∞"
    group_names = list(set(msg.channel.lstrip('@') for msg in messages))
    community_name = ', '.join(group_names)
    header = f"<b>üë• –û–±–∑–æ—Ä —Å–æ–æ–±—â–µ—Å—Ç–≤–∞ {community_name}</b>\n\n"
    
    result = header + result
    print("group_result:", "="*100, "\n", result, "\n", "="*100, "\n")
    return result


async def process_messages(messages: List[MessageInfo], save_changes: bool, send_message: bool, is_group: bool = False):
    """Processes a list of messages: filters, deduplicates, summarizes, and sends."""
    from config import SOURCE_CHANNELS
    from history_manager import save_summarization_history, save_group_summarization_history, save_summary_to_history, save_group_summary_to_history, update_group_last_run
    from channel_manager import save_discovered_channel
    from telegram_client import send_message_to_target_channel
    from datetime import datetime, timezone
    
    print(f"Fetched {len(messages)} new messages from {'groups' if is_group else 'channels'}")

    if not messages:
        print(f"No new messages found in {'groups' if is_group else 'channels'}")
        return

    filtered = []
    all_checked_messages = []
    discovered_channels = set() # Only relevant for channels, but kept for consistency

    for i, msg in enumerate(messages):
        print(f"Checking message {i+1}/{len(messages)}...")
        all_checked_messages.append(msg)

        if await is_nlp_related(msg.text):
            filtered.append(msg)
            if not is_group and msg.channel not in SOURCE_CHANNELS:
                discovered_channels.add(msg.channel)
            print(f"  ‚úì Message {i+1} is NLP-related: {msg.text[:100]}; {msg.link}")
        else:
            print(f"  ‚úó Message {i+1} is not NLP-related (likely advertising): {msg.text[:100]}; "
                  f"{msg.channel}; {msg.link}")
    print(f"{len(filtered)} messages after NLP filter")

    if save_changes:
        if is_group:
            save_group_summarization_history(all_checked_messages)
            print(f"Saved {len(all_checked_messages)} checked group messages to history")
        else:
            save_summarization_history(all_checked_messages)
            print(f"Saved {len(all_checked_messages)} checked messages to history")

        if not is_group: # Only save discovered channels for non-group messages
            for channel in discovered_channels:
                save_discovered_channel(channel)
            if discovered_channels:
                print(f"Discovered {len(discovered_channels)} new channels: {', '.join(discovered_channels)}")

    if filtered:
        unique = await (remove_group_duplicates(filtered) if is_group else remove_duplicates(filtered))
        print(f"{len(unique)} messages after deduplication")

        for msg in unique:
            if ENABLE_SUMMARIES_DEDUPLICATION:
                if await is_message_covered_in_summaries(msg):
                    await process_covered_message(msg)
                elif await is_message_covered_in_group_summaries(msg):
                    await process_covered_message(msg, is_group=True)

        if unique:
            summary = await (summarize_group_text(unique) if is_group else summarize_text(unique))
            if send_message:
                await send_message_to_target_channel(summary)
                print(f"{'Group' if is_group else 'Channel'} summary sent")

            if save_changes:
                channels = list(set(msg.channel for msg in unique))
                summary_info = SummaryInfo(
                    content=summary,
                    date=datetime.now(timezone.utc),
                    message_count=len(unique),
                    channels=channels
                )
                if is_group:
                    save_group_summary_to_history(summary_info)
                    print(f"Group summary saved to history (groups: {channels})")
                    update_group_last_run()
                    print("Group summarization completed for today")
                else:
                    save_summary_to_history(summary_info)
                    print(f"Channel summary saved to history (channels: {channels})")
        else:
            print(f"No unique NLP messages found in {'groups' if is_group else 'channels'}")
    else:
        print(f"No new NLP-related messages found in {'groups' if is_group else 'channels'}")


async def process_covered_message(msg: MessageInfo, is_group: bool = False):
    """Process a message that is already covered in previous summaries."""
    from config import ENABLE_SUMMARY_UPDATES
    from history_manager import find_relevant_summary_for_update, update_existing_summary, save_updated_summary
    
    if not ENABLE_SUMMARY_UPDATES:
        print(f"  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –æ—Å–≤–µ—â–µ–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ: {msg.text[:50]}...")
        return
    
    print(f"  –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å–∞–º–º–∞—Ä–∏ –¥–ª—è: {msg.text[:50]}...")
    
    # –ò—â–µ–º –ø–æ–¥—Ö–æ–¥—è—â–µ–µ —Å–∞–º–º–∞—Ä–∏ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
    relevant_summary = await find_relevant_summary_for_update(msg, is_group)
    if relevant_summary:
        print(f"  –ù–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–µ–µ —Å–∞–º–º–∞—Ä–∏ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è")
        print("relevant_summary:", "="*100, "\n", relevant_summary, "\n", "="*100, "\n")
        updated_summary = await update_existing_summary(relevant_summary, msg, is_group)
        print("updated_summary:", "="*100, "\n", updated_summary, "\n", "="*100, "\n")
        if updated_summary:
            await save_updated_summary(updated_summary, is_group)
            print(f"  –°–∞–º–º–∞—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–æ —Å –Ω–æ–≤—ã–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º")
        else:
            print(f"  –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±–Ω–æ–≤–∏—Ç—å —Å–∞–º–º–∞—Ä–∏")
    else:
        print(f"  –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–µ–µ —Å–∞–º–º–∞—Ä–∏ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è")
